{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38583105",
   "metadata": {},
   "source": [
    "# Assignment 4: Attention\n",
    "- The assignment is still under construction.\n",
    "- But still, you can solve Problems 1 and 2\n",
    "- If you find any error, please do not hesitate to report or make a question on Cyber Campus\n",
    "    - Don't waste too much time on solving the error. The code is not thoroughly checked, and the error can be not your fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are in Colab, install transformers \n",
    "!pip -q install transformers\n",
    "!pip install koreanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import os\n",
    "\n",
    "# Below helps to run tokenizer with multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecd6024f",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182b757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the example of vectorization of dot product for two different sequence length.\n",
    "'''\n",
    "\n",
    "e_states = torch.randn(100, 16)\n",
    "d_states = torch.randn(80, 16)\n",
    "\n",
    "dot_product = torch.mm(e_states, d_states.permute(1,0)) # (100, 16) x (16, 80) = (100, 80)\n",
    "dot_product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcfefeae",
   "metadata": {},
   "source": [
    "## Problem 1: Implement Dot Product Attention\n",
    "\n",
    "- Optimizing computation time is really important\n",
    "    - Use `torch.mm()` or `torch.matmul()`\n",
    "    - `torch.mm(a, b)` is a function for calculating matrix multiplcation of two matrices `a` and `b`\n",
    "        - `a` and `b` has to be 2-dim tensors\n",
    "        - `a.shape[1]` has to be equal to `b.shape[0]`\n",
    "    - `torch.matmul()` is a function for matrix multiplication but with broadcasting\n",
    "        - https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "        - It has less restriction on its input shape.\n",
    "            - It automatically matches the dimension of two tensors following some rules\n",
    "            - Therefore, it is a bit risky to use this funciton if you don't understand how it works\n",
    "- **DO NOT** use element-wise product or for loop!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2059e903",
   "metadata": {},
   "source": [
    "### Hint: Dot product as matrix multiplcation.\n",
    "\n",
    "- Let's say there are two vector, $u=\\begin{bmatrix}-3 \\\\ 2 \\\\ 1\\end{bmatrix}$ and $v = \\begin{bmatrix} 5 \\\\ 4 \\\\ 6\\end{bmatrix}$\n",
    "    - The dot product of the two vectors is $(-3 \\times 5) + (2 \\times 4) + (1 \\times 6) = 1$\n",
    "    - It is equivalent to $u^T \\times v$\n",
    "        - In this case $u\\in\\mathbb{R}^{3\\times1}$ and $v\\in\\mathbb{R}^{3\\times1}$\n",
    "- In PyTorch, this can be described as below:\n",
    "    - `u = torch.Tensor([-3, 2, 1])`\n",
    "    - `v = torch.Tensor([5, 4, 6])`\n",
    "    - Dot product of u and v can be calculated by one of belows:\n",
    "        - `torch.mm(u.unsqueeze(0), v.unsqueeze(1))`\n",
    "            - `u.unsqueeze(0).shape == [1, 3]`\n",
    "            - `v.unsqueeze(1).shape == [3, 1]`\n",
    "            - `unsqueeze()` returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "            - The result has shape of [1,1]\n",
    "        - `torch.matmul(u, v)`\n",
    "        - `u @ v`\n",
    "            - `@` denotes matrix multiplication, which was introduced from Python 3.5\n",
    "        - `(u * v).sum()`\n",
    "            - This will be much slower than others, because it first do element-wise multiplcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aed8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hint: Dot product as matrix multiplcation.\n",
    "'''\n",
    "\n",
    "u = torch.Tensor([-3, 2, 1])\n",
    "v = torch.Tensor([5, 4, 6])\n",
    "\n",
    "print(f\"Result of (u * v).sum() is {(u * v).sum()}. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\") \n",
    "print(f\"Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is {torch.mm(u.unsqueeze(0), v.unsqueeze(1))}\")\n",
    "print(f\"Result of torch.matmul(u, v) is {torch.matmul(u, v)}\")\n",
    "print(f\"Result of u @ v is {u @ v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a20b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_score_for_a_single_query(keys, query):\n",
    "  '''\n",
    "  This function returns an attention score for each vector in keys for a given query.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [T, C]. These are vectors that a query wants attend to\n",
    "    query (torch.Tensor): Has a shape of [C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [T]\n",
    "                                    \n",
    "    attention_score[i] has to be a dot product value between keys[i] and query                                 \n",
    "\n",
    "\n",
    "  TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
    "  Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_t = 23\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_t, h_size)\n",
    "query = torch.randn(h_size)\n",
    "\n",
    "att_score = get_attention_score_for_a_single_query(keys, query)\n",
    "att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Case\n",
    "'''\n",
    "assert att_score.ndim == 1 and len(att_score) == num_t, \"Error: Check output shape\"\n",
    "answer = torch.Tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
    "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
    "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])\n",
    "assert torch.allclose(att_score, answer, atol=1e-4) , \"Error: The output value is different\"\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weight_from_score(attention_score):\n",
    "  '''\n",
    "  This function converts attention score to attention weight.\n",
    "  \n",
    "  Argument:\n",
    "    attention_score (torch.Tensor): Tensor of real number. Has a shape of [T]\n",
    "\n",
    "  Output:\n",
    "    attention_weight (torch.Tensor): Tensor of real number between 0 and 1. Sum of attention_weight is 1. Has a shape of [T]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  assert attention_score.ndim == 1\n",
    "  \n",
    "  return\n",
    "\n",
    "att_weight = get_attention_weight_from_score(att_score)\n",
    "att_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = torch.Tensor([0.0000,     0.0013,     0.0009,     0.0000,     0.0041,     0.0002,\n",
    "            0.0001,     0.0000,     0.0002,     0.0028,     0.0001,     0.0001,\n",
    "            0.0002,     0.0001,     0.0004,     0.0000,     0.0000,     0.0000,\n",
    "            0.0000,     0.0022,     0.0001,     0.9756,     0.0118])\n",
    "assert att_weight.shape == att_score.shape, 'Shape has to be remained the same'\n",
    "assert att_weight.sum() == 1, \"Sum of attention weight has to be 1\"\n",
    "assert torch.allclose(att_weight, answer, atol=1e-4) , \"Error: The output value is different\"\n",
    "\n",
    "print(\"Passed all the cases!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sum(values, attention_weight):\n",
    "  '''\n",
    "  This function converts attention score to attention weight\n",
    "  \n",
    "  Argument:\n",
    "    values (torch.Tensor): Has a shape of [T, C]. These are vectors that are used to form attention vector\n",
    "    attention_weight: Has a shape of [T], which represents the weight for each vector to compose the attention vector\n",
    "\n",
    "  Output:\n",
    "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
    "  \n",
    "  TODO: Complete this function using torch.mm\n",
    "  '''\n",
    "  return\n",
    "\n",
    "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
    "att_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = torch.Tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
    "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])\n",
    "assert att_vec.shape == query.shape, 'Shape has to be remained the same'\n",
    "assert torch.allclose(att_vec, answer, atol=1e-4) , \"Error: The output value is different\"\n",
    "print(\"Passed all the cases\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc00df77",
   "metadata": {},
   "source": [
    "## Problem 2: Attention in Batch ( 16 pts)\n",
    "- In this problem, you have to calculate attention with batch\n",
    "- You can use `torch.bmm()` for batch matrix multiplication https://pytorch.org/docs/stable/generated/torch.bmm.html \n",
    "    - `torch.bmm()` takes two 3-dim tensor as its input\n",
    "    - Each tensor has to be 3-dim (atensor.ndim==3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062446fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hint for Problem 2\n",
    "\n",
    "You can calculate matrix multiplication of matrices in batch effectively using torch.bmm() or torch.matmul()\n",
    "'''\n",
    "\n",
    "torch.manual_seed(0)\n",
    "matrix_left1 = torch.randn(5, 3)\n",
    "matrix_left2 = torch.randn(5, 3)\n",
    "\n",
    "print(f\"matrix_left1: \\n{matrix_left1}\")\n",
    "print(f\"matrix_left2: \\n{matrix_left2}\")\n",
    "\n",
    "matrix_right1 = torch.randn(3, 4)\n",
    "matrix_right2 = torch.randn(3, 4)\n",
    "print(f\"matrix_right1: \\n{matrix_right1}\")\n",
    "print(f\"matrix_right2: \\n{matrix_right2}\")\n",
    "\n",
    "print(\"Let's assume that we have batch of matrix, which is stack of these two matices\")\n",
    "matrix_left = torch.stack([matrix_left1, matrix_left2])\n",
    "matrix_right = torch.stack([matrix_right1, matrix_right2])\n",
    "\n",
    "print(f\"matrix_left: \\n{matrix_left} \\n which is shape of {matrix_left.shape}\")\n",
    "print(f\"matrix_right: \\n{matrix_right}\\n which is shape of {matrix_right.shape}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Exhaustive method: using torch.mm() only with for loop (This is SLOW when matrix gets much larger)\n",
    "'''\n",
    "\n",
    "mm_forloop_output = []\n",
    "for sample_index in range(matrix_left.shape[0]):\n",
    "  mat_left = matrix_left[sample_index]\n",
    "  mat_right = matrix_right[sample_index]\n",
    "  \n",
    "  mm_result = torch.mm(mat_left, mat_right)\n",
    "  mm_forloop_output.append(mm_result)\n",
    "  \n",
    "mm_forloop_stack = torch.stack(mm_forloop_output)\n",
    "print(f\"mat_mul_stack: \\n{mm_forloop_stack}\\n which is shape of {mm_forloop_stack.shape}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Good method: using torch.bmm()\n",
    "'''\n",
    "\n",
    "mat_mul_bmm = torch.bmm(matrix_left, matrix_right)\n",
    "print(f\"mat_mul_bmm: \\n{mat_mul_bmm}\\n which is shape of {mat_mul_bmm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe689641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_score_for_a_batch_query(keys, query):\n",
    "  '''\n",
    "  This function returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [N, T, C]. These are vectors that a query wants attend to\n",
    "    query (torch.Tensor): Has a shape of [N, C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, T]\n",
    "                                    \n",
    "    attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]                     \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
    "\n",
    "  '''\n",
    "  return \n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_b = 6\n",
    "num_t = 23\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_b,num_t, h_size)\n",
    "query = torch.randn(num_b, h_size)\n",
    "out = get_attention_score_for_a_batch_query(keys, query)\n",
    "\n",
    "assert out.ndim == 2 and out.shape == torch.Size([num_b, num_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30995b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
    "  '''\n",
    "  Now you have to implement the attention score for not only single query, but multiple queries.\n",
    "  \n",
    "  This function returns a batch of attention score for each vector in keys for given queries.\n",
    "  You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
    "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
    "  \n",
    "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
    "  \n",
    "  Arguments:\n",
    "    keys (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that a query wants attend to\n",
    "    queries (torch.Tensor): Has a shape of [N, Tt, C]. This is a vector that attends to other set of vectors (keys and values)\n",
    "  \n",
    "  Output:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "                                    \n",
    "    attention_score[n, i, t] has to be a dot product value between keys[n, i] and query[n, t] \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
    "        Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
    "        and how the result tensor has to look like (shape)\n",
    "  '''\n",
    "  return\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_b = 6\n",
    "num_ts = 23\n",
    "num_tt = 14\n",
    "h_size = 16\n",
    "\n",
    "keys = torch.randn(num_b, num_ts, h_size)\n",
    "queries = torch.randn(num_b, num_tt, h_size)\n",
    "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
    "\n",
    "att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb45f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
    "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
    "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
    "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
    "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
    "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_ts, num_tt]), 'Check the output shape'\n",
    "assert torch.max(torch.abs(att_score[2,4] - answer)) < 1e-4, 'Calculated result is wrong'\n",
    "assert torch.max(torch.abs(att_score[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
    "\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef2611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_masked_softmax(attention_score, mask, mask_value=-1e10):\n",
    "  '''\n",
    "  During the batch computation, each sequence in the batch can have different length.\n",
    "  To group them as in a single tensor, we usually pad values\n",
    "    \n",
    "  Arguments:\n",
    "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
    "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
    "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
    "  \n",
    "  Output:\n",
    "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
    "                                    Has a shape of [N, Ts, Tt]\n",
    "                                    \n",
    "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t] \n",
    "    \n",
    "  TODO: Complete this function without using for loop\n",
    "  Hint: You can give -infinity value by -float(\"inf\")\n",
    "\n",
    "  '''\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "'''\n",
    "Don't change this codes\n",
    "'''\n",
    "mask = torch.ones_like(att_score)[..., 0]\n",
    "mask[4, 15:] = 0\n",
    "mask[5, 17:] = 0\n",
    "\n",
    "attention_weight = get_masked_softmax(att_score, mask)\n",
    "attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = torch.Tensor([0.0120,     0.0002,     0.0901,     0.0003,     0.0259,     0.0036,\n",
    "            0.5617,     0.0108,     0.2508,     0.0054,     0.0001,     0.0010,\n",
    "            0.0000,     0.0005,     0.0375,     0.0000,     0.0000,     0.0000,\n",
    "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000])\n",
    "assert torch.max(torch.abs(attention_weight[4,:,3]-answer)) < 1e-4\n",
    "assert torch.max(torch.abs(attention_weight.sum(1) -  1 )) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e5c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batch_weighted_sum(values, attention_weight):\n",
    "  '''\n",
    "  This function converts attention score to attention weight\n",
    "  \n",
    "  Argument:\n",
    "    values (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that are used to form attention vector\n",
    "    attention_weight: Has a shape of [N, Ts, Tt], which represents the weight for each vector to compose the attention vector\n",
    "                      attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
    "\n",
    "  Output:\n",
    "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. \n",
    "                                     Has a shape of [N, Tt, C]\n",
    "  \n",
    "  TODO: Complete this function using torch.bmm\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
    "att_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93231ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([-0.9348, -1.2628, -0.9189, -0.3434, -1.6476,  0.1031, -0.6963, -0.7462,\n",
    "         0.1484,  0.6810,  0.7950,  1.0277, -1.5988,  0.4232, -1.5540,  0.1801])\n",
    "answer2 = torch.Tensor([-0.9204, -0.9710,  0.3062, -1.0122,  1.1933,  0.1302, -1.0280,  0.0095,\n",
    "         0.6124,  0.0615, -1.2312, -0.6714, -0.1764, -0.1254])\n",
    "assert att_out.ndim == 3 and att_out.shape == torch.Size([num_b, num_tt, h_size]), 'Check the output shape'\n",
    "assert torch.max(torch.abs(att_out[2, 5] - answer)) < 1e-4, 'Calculated result is wrong'\n",
    "assert torch.max(torch.abs(att_out[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
    "\n",
    "print(\"Passed all the cases!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbdbf2b",
   "metadata": {},
   "source": [
    "## Problem 3: Make seq2seq with attention (14 pts)\n",
    "- Using Pre-defined `TranslatorBi` class, complete a new `TranslatorAtt` class\n",
    "- If you implement it correctly, you can translate "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904d44a3",
   "metadata": {},
   "source": [
    "### 3-0 Prepare dataset and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c670709",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download dataset (originally from NIA AI-Hub)\n",
    "'''\n",
    "\n",
    "!gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
    "!unzip -q nia-aihub-korean-english.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('nia_korean_english')\n",
    "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
    " \n",
    "# Use only first two xlsx files in the assignment\n",
    "data_list = data_list[:2]\n",
    "df = pd.concat([pd.read_excel(path) for path in data_list], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f310bde",
   "metadata": {},
   "source": [
    "### Download Pretrained Weights, and Tokenizers\n",
    "To use the pretrained model correctly, you can use the pretrained vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1lTo32Z9euLMSD1L1krgORceay9f-UU--\n",
    "!unzip nlp_assignment4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093978a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for path in data_list:\n",
    "  df = pd.read_excel(path)\n",
    "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
    "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
    "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
    "      f.write('\\n'.join(df['원문']))\n",
    "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
    "      f.write('\\n'.join(df['번역문']))\n",
    "\n",
    "\n",
    "# Train Tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
    "\n",
    "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
    "limit_alphabet= 6000   \n",
    "min_frequency = 5 \n",
    "\n",
    "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list]\n",
    "output_dir   = Path('hugging_kor_partial_%d'%(vocab_size))\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n",
    "\n",
    "tokenizer.save_model(str(output_dir))\n",
    "\n",
    "limit_alphabet= 200\n",
    "corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list]\n",
    "output_dir   = Path('hugging_eng_partial_%d'%(vocab_size))\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n",
    "\n",
    "tokenizer.save_model(str(output_dir))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7061145",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
    "                                                       strip_accents=False,\n",
    "                                                       lowercase=False) \n",
    "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
    "                                                       strip_accents=False,\n",
    "                                                       lowercase=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationSet:\n",
    "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
    "    self.data = df[ ['원문', '번역문']].values\n",
    "    self.src_tokenizer = src_tokenizer\n",
    "    self.tgt_tokenizer = tgt_tokenizer\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    src_str = self.data[idx, 0]\n",
    "    tgt_str = self.data[idx, 1]\n",
    "\n",
    "    # convert string to list of token ids\n",
    "    src_ids = self.src_tokenizer.encode(src_str)\n",
    "    tgt_ids = self.tgt_tokenizer.encode(tgt_str)\n",
    "\n",
    "    return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids) # idx-th datasample\n",
    "  \n",
    "entireset = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
    "trainset, validset, testset = torch.utils.data.random_split(entireset, [int(len(entireset)*0.9), int(len(entireset)*0.05), len(entireset)-int(len(entireset)*0.9)-int(len(entireset)*0.05)], generator=torch.Generator().manual_seed(42))\n",
    "# trainset, validset, testset = torch.utils.data.random_split(entireset, [360000, 20000, 20000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f'Dataset Item Example: {entireset[0]}')\n",
    "print(f'Length of split : Train {len(trainset)}, Valid {len(validset)}, Test {len(testset)}')\n",
    "\n",
    "def pack_collate(raw_batch):\n",
    "  srcs = [x[0] for x in raw_batch]\n",
    "  tgts_i = [x[1][:-1] for x in raw_batch]\n",
    "  tgts_o = [x[1][1:] for x in raw_batch]\n",
    "  \n",
    "  srcs = pack_sequence(srcs, enforce_sorted=False)\n",
    "  tgts_i = pack_sequence(tgts_i, enforce_sorted=False)\n",
    "  tgts_o = pack_sequence(tgts_o, enforce_sorted=False)\n",
    "  return srcs, tgts_i, tgts_o\n",
    "\n",
    "single_loader = DataLoader(trainset, batch_size=1, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined class\n",
    "'''\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device, model_name='nmt_model'):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_fn = loss_fn\n",
    "    self.train_loader = train_loader\n",
    "    self.valid_loader = valid_loader\n",
    "    \n",
    "    self.model.to(device)\n",
    "    \n",
    "    self.grad_clip = 1.0\n",
    "    self.best_valid_accuracy = 0\n",
    "    self.device = device\n",
    "    \n",
    "    self.training_loss = []\n",
    "    self.validation_loss = []\n",
    "    self.validation_acc = []\n",
    "    self.model_name = model_name\n",
    "\n",
    "  def save_model(self, path):\n",
    "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
    "    \n",
    "  def train_by_num_epoch(self, num_epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "      self.model.train()\n",
    "      for batch in tqdm(self.train_loader, leave=False):\n",
    "        loss_value = self._train_by_single_batch(batch)\n",
    "        self.training_loss.append(loss_value)\n",
    "      self.model.eval()\n",
    "      validation_loss, validation_acc = self.validate()\n",
    "      self.validation_loss.append(validation_loss)\n",
    "      self.validation_acc.append(validation_acc)\n",
    "      \n",
    "      if validation_acc > self.best_valid_accuracy:\n",
    "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
    "        self.save_model(f'{self.model_name}_best.pt')\n",
    "      else:\n",
    "        self.save_model(f'{self.model_name}_last.pt')\n",
    "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
    "\n",
    "      \n",
    "  def _train_by_single_batch(self, batch):\n",
    "    '''\n",
    "    This method updates self.model's parameter with a given batch\n",
    "    \n",
    "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
    "    \n",
    "    You have to use variables below:\n",
    "    \n",
    "    self.model (Translator/torch.nn.Module): A neural network model\n",
    "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
    "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
    "    self.device (str): 'cuda' or 'cpu'\n",
    "\n",
    "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
    "    The model's parameters, optimizer's steps has to be updated inside this method\n",
    "    '''\n",
    "    \n",
    "    src, tgt_i, tgt_o = batch\n",
    "    pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
    "    loss = self.loss_fn(pred.data, tgt_o.data)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "    self.optimizer.step()\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "    \n",
    "  def validate(self, external_loader=None):\n",
    "    '''\n",
    "    This method calculates accuracy and loss for given data loader.\n",
    "    It can be used for validation step, or to get test set result\n",
    "    \n",
    "    input:\n",
    "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
    "      \n",
    "    output: \n",
    "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
    "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
    "    '''\n",
    "    \n",
    "    ### Don't change this part\n",
    "    if external_loader and isinstance(external_loader, DataLoader):\n",
    "      loader = external_loader\n",
    "      print('An arbitrary loader is used instead of Validation loader')\n",
    "    else:\n",
    "      loader = self.valid_loader\n",
    "      \n",
    "    self.model.eval()\n",
    "    \n",
    "    '''\n",
    "    Write your code from here, using loader, self.model, self.loss_fn.\n",
    "    '''\n",
    "    validation_loss = 0\n",
    "    validation_acc = 0\n",
    "    num_total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in tqdm(loader, leave=False):\n",
    "        \n",
    "        src, tgt_i, tgt_o = batch\n",
    "        pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
    "        loss = self.loss_fn(pred.data, tgt_o.data)\n",
    "        num_tokens = tgt_i.data.shape[0]\n",
    "        validation_loss += loss.item() * num_tokens\n",
    "        num_total_tokens += num_tokens\n",
    "        \n",
    "        acc = torch.sum(torch.argmax(pred.data, dim=-1) == tgt_o.to(self.device).data)\n",
    "        validation_acc += acc.item()\n",
    "        \n",
    "    return validation_loss / num_total_tokens, validation_acc / num_total_tokens\n",
    "\n",
    "def get_nll_loss(predicted_prob_distribution, indices_of_correct_token, eps=1e-10):\n",
    "  '''\n",
    "  for PackedSequence, the input is 2D tensor\n",
    "  \n",
    "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
    "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
    "  '''\n",
    "  prob_of_correct_next_word = predicted_prob_distribution[torch.arange(len(predicted_prob_distribution)), indices_of_correct_token]\n",
    "  loss = -torch.log(prob_of_correct_next_word+eps)\n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a011e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined class\n",
    "\n",
    "You don't need to change this code\n",
    "'''\n",
    "class TranslatorBi(nn.Module):\n",
    "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
    "    super().__init__()\n",
    "    self.src_tokenizer = src_tokenizer\n",
    "    self.tgt_tokenizer = tgt_tokenizer\n",
    "    \n",
    "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
    "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
    "    \n",
    "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
    "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
    "    \n",
    "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    \n",
    "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
    "    \n",
    "  def run_encoder(self, x):\n",
    "    if isinstance(x, PackedSequence):\n",
    "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
    "    else:\n",
    "      emb_x = self.src_embedder(x)\n",
    "      \n",
    "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
    "    \n",
    "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
    "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
    "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
    "    if isinstance(x, PackedSequence):\n",
    "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
    "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
    "    else:\n",
    "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
    "      \n",
    "    \n",
    "    return enc_hidden_state_by_t, last_hidden_sum \n",
    "\n",
    "  def run_decoder(self, y, last_hidden_state):\n",
    "    if isinstance(y, PackedSequence):\n",
    "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
    "    else:\n",
    "      emb_y = self.tgt_embedder(y)\n",
    "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
    "    return out, decoder_last_hidden\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    '''\n",
    "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
    "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
    "    '''\n",
    "    \n",
    "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
    "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
    "    \n",
    "    if isinstance(out, PackedSequence):\n",
    "      logits = self.decoder_proj(out.data)\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
    "    else:\n",
    "      logits = self.decoder_proj(out)\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2060d164",
   "metadata": {},
   "source": [
    "### Problem 3.1: Complete the Seq2Seq with Attention\n",
    "- **Caution**: You have to concatenate [decoder_hidden_state; attention_out] for this implementation\n",
    "    - You can use different order of concatenation, but the pre-trained model used that specific order, so please follow it so that you can use the pre-trained weight correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e285f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TranslatorAtt(TranslatorBi):\n",
    "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
    "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
    "    \n",
    "    # TODO: define new self.decoder_proj\n",
    "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
    "    \n",
    "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
    "    '''\n",
    "    Arguments:\n",
    "      encoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of encoder GRU. Shape: [N, Ts, C]\n",
    "      decoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of decoder GRU. Shape: [N, Tt, C]\n",
    "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Tt, Ts]\n",
    "\n",
    "    Outputs:\n",
    "      attention_vectors (torch.Tensor or PackedSequence): Attention vectors that has the same shape as decoder_hidden_states\n",
    "      attention_weights (torch.Tensor): Zero-padded attention weights.\n",
    "                                You don't need to return it during the training, but it will help you to implement later problem\n",
    "    \n",
    "    TODO: Complete this function using following functions\n",
    "      get_attention_score_for_a_batch_multiple_query\n",
    "      get_masked_softmax\n",
    "      get_batch_weighted_sum\n",
    "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
    "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True) to convert PackedSequence to Tensor\n",
    "    Use torch.nn.utils.rnn.pack_padded_sequence(tensor, batch_lens, batch_first=True) to convert Tensor to PackedSequence\n",
    "    '''\n",
    "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
    "    if is_packed:\n",
    "      encoder_hidden_states, source_lens = pad_packed_sequence(encoder_hidden_states, batch_first=True)\n",
    "      decoder_hidden_states, target_lens = pad_packed_sequence(decoder_hidden_states, batch_first=True)\n",
    "    \n",
    "    # Write your code from here\n",
    "\n",
    "    # 1. Calculate attention score using encoder_hidden_states and decoder_hidden_states\n",
    "    # 2. Mask the attention score using mask and apply softmax to get attention weight\n",
    "    # 3. Calculate attention vector using attention weight and encoder_hidden_states\n",
    "\n",
    "    \n",
    "    # \n",
    "\n",
    "\n",
    "    return \n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    '''\n",
    "    Arguments:\n",
    "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
    "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
    "    Output:\n",
    "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "\n",
    "    is_packed = isinstance(x, PackedSequence)\n",
    "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
    "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
    "    \n",
    "    if is_packed:\n",
    "      mask = pad_packed_sequence(x, batch_first=True)[0] != 0\n",
    "    else:\n",
    "      mask = torch.ones(x.shape[0], x.shape[1])\n",
    "\n",
    "    attention_vec = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
    "\n",
    "    # TODO: Write your code from here\n",
    "    # CAUTION: \n",
    "    #   For the concatenation, you have to concat [dec_hidden_state_by_t; attention_vec], not [attention_vec; dec_hidden_state_by_t]\n",
    "    return\n",
    "  \n",
    "\n",
    "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
    "\n",
    "model(batch[0], batch[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3212344b",
   "metadata": {},
   "source": [
    "#### Test your model\n",
    "- To evaluate your implementation, you have to load the pretrained weight of the same model.\n",
    "- If your implementation is correct, the resulting value would be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b291ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weight\n",
    "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
    "state_dict = torch.load('nmt_attention_512_grad1_lr1e-4_best.pt', map_location='cpu')['model']\n",
    "model.eval()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Load the pre-calculated example and result\n",
    "prob3_values = torch.load('assignment_4_values.pt')\n",
    "single_batch_example, packed_batch_example, correct_single_out, correct_packed_out = prob3_values['single_test_batch'], prob3_values['packed_test_batch'], prob3_values['single_test_out'],  prob3_values['packed_test_out'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Case for Single-size Batch\n",
    "'''\n",
    "single_out = model(single_batch_example[0], single_batch_example[1])\n",
    "\n",
    "assert isinstance(single_out, torch.Tensor), \"The output of model for Tensor has to be Tensor\"\n",
    "assert torch.max(torch.abs(single_out - correct_single_out)) < 1e-5, \"The output value is different from the expected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Case for Batch with PackedSequence\n",
    "'''\n",
    "packed_out = model(packed_batch_example[0], packed_batch_example[1])\n",
    "\n",
    "assert isinstance(packed_out, PackedSequence), \"The output of model for PackedSequence has to be PackedSequence\"\n",
    "assert (packed_out.batch_sizes == correct_packed_out.batch_sizes).all(), \"Output's batch_sizes is wrong\"\n",
    "assert (packed_out.sorted_indices == correct_packed_out.sorted_indices).all(), \"Output's sorted_indices is wrong\"\n",
    "\n",
    "assert torch.max(torch.abs(packed_out.data - correct_packed_out.data)) < 1e-5,  \"The output value is different from the expected\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e0ab23",
   "metadata": {},
   "source": [
    "### Train the model (Optional)\n",
    "- You can try to train your model, but you can just load the pretrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainer = Trainer(model, optimizer, get_nll_loss, train_loader, valid_loader, 'cuda', 'nmt_attention_512')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cea22364",
   "metadata": {},
   "source": [
    "### Problem 3.2: Implement Inference with Attention Weights\n",
    "- In this problem, you have to implement an inference code that returns translation for given source sentence, but also **attention weights** between source sentence and target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, source_sentence):\n",
    "  '''\n",
    "  \n",
    "  Arguments:\n",
    "    model (TranslatorAtt): Translator model with attention\n",
    "    source_sentence (str): Sentence to translate\n",
    "\n",
    "  Returns:\n",
    "    input_tokens (list): Source sentence in a list of token in token_id\n",
    "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
    "    decoded_string (str): Translated sentence in string\n",
    "    attention_map (torch.Tensor): Attention weight between each token of source sentence and target sentence. Has a shape of [Ts, Tt]\n",
    "    \n",
    "  '''\n",
    "  \n",
    "  input_tokens = model.src_tokenizer.encode(source_sentence)\n",
    "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
    "  mask = torch.ones_like(input_tensor)\n",
    "  enc_hidden_state_by_t, last_hidden_sum = model.run_encoder(input_tensor)\n",
    "  \n",
    "  # Setup for 0th step\n",
    "  current_hidden = last_hidden_sum\n",
    "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
    "  total_output = []\n",
    "  total_attetion_weights = []\n",
    "\n",
    "  for i in range(100): # You can chage it to while True:\n",
    "    emb = model.tgt_embedder(current_decoder_token)\n",
    "    '''\n",
    "    TODO: Complete the code here\n",
    "    \n",
    "    You have to \n",
    "      1) run decoder rnn for a single step\n",
    "      2) get attention weight (variable name: att_weight) and attention vector.\n",
    "         att_weight.shape == torch.Size([1, len(tokenized_sentence), 1])\n",
    "      3) concat decoder out and attention vector\n",
    "      4) calculate probabilty logit (variable name: logit)\n",
    "    '''\n",
    "\n",
    "\n",
    "    # You don't have to change the codes below.\n",
    "    # Declare logit and last_hidden properly so that the code below can run without error\n",
    "    selected_token = torch.argmax(logit, dim=-1)\n",
    "    current_decoder_token = selected_token\n",
    "    current_hidden = last_hidden\n",
    "    if current_decoder_token == 3: ## end of sentence token\n",
    "      break\n",
    "    total_output.append(selected_token[0])\n",
    "    total_attetion_weights.append(att_weight[0,:,0])\n",
    "  predicted_tokens = torch.cat(total_output, dim=0).tolist()\n",
    "  attention_map = torch.stack(total_attetion_weights, dim=1)\n",
    "  \n",
    "  return  input_tokens, predicted_tokens, model.tgt_tokenizer.decode(predicted_tokens), attention_map\n",
    "\n",
    "model.cpu()\n",
    "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.')\n",
    "print(translated_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e466b4f4",
   "metadata": {},
   "source": [
    "### Plot attention map\n",
    "- If you completed `translate()`, you can visualize the result of attention weight as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(len(pred_tokens)*0.8, len(input_tokens)*0.8))\n",
    "x_axis_label = [model.tgt_tokenizer.decode(x) for x in pred_tokens]\n",
    "y_axis_label = [model.src_tokenizer.decode(x) for x in input_tokens]\n",
    "\n",
    "plt.imshow(att_weights.detach())\n",
    "plt.xticks(range(len(x_axis_label)), x_axis_label, fontsize=15,rotation = 45)\n",
    "plt.yticks(range(len(y_axis_label)), y_axis_label, fontsize=15)\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7292462",
   "metadata": {},
   "source": [
    "## Problem 4: Self Attention (8 pts)\n",
    "- In this problem, you will implement the key-query-value calculation that was used for Transformer\n",
    "- Also, you have to implement simple self-attention (without multiheaded attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_query_value(input_tensor, kqv_layer):\n",
    "  '''\n",
    "  This function returns key, query, and value that is calculated by input tensor and nn_layer.\n",
    "\n",
    "  Arguments:\n",
    "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
    "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
    "    \n",
    "  Outputs:\n",
    "    keys (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    queries (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    values (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "    \n",
    "  Hint: Use torch.chunk() to split a tensor into given number of chunks\n",
    "  '''\n",
    "  return \n",
    "\n",
    "torch.manual_seed(0)\n",
    "test = torch.randn(4, 17, 8)\n",
    "linear = nn.Linear(8, 16 * 3)\n",
    "keys, queries, values = get_key_query_value(test, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([-0.6166,  0.2079, -0.0225, -0.2324,  0.0254,  0.0093,  0.2242, -0.4207,\n",
    "         0.1735, -0.3859,  0.1021, -0.4263,  0.6088,  0.2397,  0.7548,  0.0349])\n",
    "answer2 = torch.Tensor([[ 0.8704, -0.2256,  0.6611,  0.0332, -0.5233, -0.1159,  0.1805,  0.7238,\n",
    "         0.5590,  0.7260,  1.3096,  0.2465,  1.1961,  0.1751, -0.9674,  0.6297]])\n",
    "assert keys.ndim == queries.ndim == values.ndim == 3\n",
    "assert keys.shape == queries.shape == values.shape == torch.Size([4, 17, 16])\n",
    "assert not (keys==queries).any() and not (keys==values).any() and not (values==queries).any()\n",
    "assert torch.max(torch.abs(queries[2, 13]-answer)) < 1e-4\n",
    "assert torch.max(torch.abs(values[0, 3]-answer2)) < 1e-4\n",
    "\n",
    "print('Passed all the cases!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc503bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_attention(input_tensor, kqv_layer, mask):\n",
    "  '''\n",
    "  This function returns output of self-attention for a given input tensor using with a given kqv_layer\n",
    "  \n",
    "  Arguments:\n",
    "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
    "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
    "    mask (torch.Tensor): \n",
    "    \n",
    "  Outputs:\n",
    "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "\n",
    "  TODO: Complete this function using your completed functions of below:\n",
    "        get_attention_score_for_a_batch_multiple_query()\n",
    "        get_masked_softmax()\n",
    "        get_batch_weighted_sum()\n",
    "        get_key_query_value()\n",
    "  '''\n",
    "  return\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "test = torch.randn(4, 17, 8)\n",
    "linear = nn.Linear(8, 16 * 3)\n",
    "mask = torch.ones_like(test)[..., 0]\n",
    "mask[2, 4:] = 0\n",
    "mask[3, 14:] = 0\n",
    "\n",
    "att_vecs = get_self_attention(test, linear, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602218f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test cases\n",
    "'''\n",
    "answer = torch.Tensor([-0.3316,  0.1992,  0.1699, -0.3703, -0.2126, -0.0147,  0.1185, -0.2360,\n",
    "         0.2283,  0.1729,  0.0460,  0.1587,  0.1891,  0.4584, -0.3860,  0.0854])\n",
    "answer2 = torch.Tensor([-0.9989,  0.4320,  0.0282, -0.6165, -0.0183,  0.1410,  0.6790, -1.3118,\n",
    "         0.1059, -0.7182, -0.5426,  0.1642, -0.6460,  0.8397,  0.4638,  0.1082])\n",
    "assert torch.max(torch.abs(att_vecs[3, 2]-answer)) < 1e-4\n",
    "assert torch.max(torch.abs(att_vecs[0, 11]-answer2)) < 1e-4\n",
    "\n",
    "\n",
    "print('Passed all the cases!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multihead_split(x, num_head):\n",
    "  '''\n",
    "  This function returns split tensor for multi-head attention\n",
    "\n",
    "  Arguments:\n",
    "    x (torch.Tensor): Has a shape of [N, T, C]\n",
    "    num_head (int): Number of heads\n",
    "  '''\n",
    "  assert x.shape[-1] % num_head == 0\n",
    "\n",
    "  # TODO: Complete this function\n",
    "  return x.reshape(x.shape[0], x.shape[1], num_head, -1).permute(0,2,1,3).reshape(x.shape[0] * num_head, x.shape[1], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_multi_head_attention(input_tensor, kqv_layer, mask, num_head=8):\n",
    "  '''\n",
    "  This function returns output of multi-headed self-attention for a given input tensor using with a given kqv_layer\n",
    "  \n",
    "  Arguments:\n",
    "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
    "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
    "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
    "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
    "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
    "    \n",
    "  Outputs:\n",
    "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
    "\n",
    "  TODO: Complete this function using your completed functions of below:\n",
    "        get_attention_score_for_a_batch_multiple_query()\n",
    "        get_masked_softmax()\n",
    "        get_batch_weighted_sum()\n",
    "        get_key_query_value()\n",
    "  '''\n",
    "  \n",
    "  n_batch, n_sequence, _ = keys.shape\n",
    "  nq = queries.shape[1]\n",
    "  \n",
    "  keys = self._get_multihead_split(keys)\n",
    "  queries = self._get_multihead_split(queries)\n",
    "  \n",
    "  score = torch.bmm(keys, queries.permute(0,2,1))\n",
    "  score /= self.dim_per_head ** 0.5\n",
    "  score = score.reshape(n_batch, self.num_head, n_sequence, nq).permute(0,2,3,1)\n",
    "\n",
    "\n",
    "  return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
