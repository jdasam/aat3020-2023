{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0654ab8c",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "- In this assignment, you will train a model for sentiment analysis\n",
    "    - Sentiment analysis is to predict whether the given text's sentiment is positive or negative\n",
    "    - The input is a sequence of tokens and the output is a result of sigmoid function\n",
    "- You have to submit a report (in pdf) and your code (in .py)\n",
    "    - In your report, you have to briefly explain about your code\n",
    "        - Code explanation can be very simple\n",
    "    - Also, add explanation on these problems\n",
    "        - Problem 6: Analyze the Prediction of Model \n",
    "    - You have to submit your code in .py file\n",
    "        - Copy and paste your completed code to ``Assignment2.py`` file\n",
    "- The main goal of this assignment is to implement a pipeline to train a neural network\n",
    "    - Problem 1: Building a Dataset class (6 pts)\n",
    "    - Problem 2: Build a Str2Idx2Str Converter (12 pts)\n",
    "        - Complete the function\n",
    "    - Problem 3: Implement a collate function (7 pts)\n",
    "    - Problem 4: Implement a Binary Cross Entropy Loss (5 pts)\n",
    "    - Problem 5: Complete Training Loop (12 pts)\n",
    "        - Training with a single batch\n",
    "        - Validate the model\n",
    "    - Problem 6: Analyze the Prediction of Model (18 pts)\n",
    "        - Write it in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31717fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download .py file. You have to copy and paste the completed function to this py file and submit it.\n",
    "!wget https://raw.githubusercontent.com/jdasam/aat3020-2023/main/Assignment2.py\n",
    "!wget https://raw.githubusercontent.com/jdasam/aat3020-2023/main/assignment2_pre_defined.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b088c",
   "metadata": {},
   "source": [
    "## Preparation: Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz # unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937695d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26a09b",
   "metadata": {},
   "source": [
    "### Make Datasplit\n",
    "- In typical machine learning tasks, one has to split training set and validation set\n",
    "    - Training set is to train the model's parameter\n",
    "    - Validation set is to check how model works for unseen dataset\n",
    "        - Validation set is used to optimize model's hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You don't have to change this cell\n",
    "'''\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "train_path = Path('aclImdb/train')\n",
    "test_path = Path('aclImdb/test')\n",
    "\n",
    "def get_train_txt_paths_in_split(dir_path='aclImdb/train', seed=0):\n",
    "  dir_path = Path(dir_path)\n",
    "  train_set, valid_set = [], []\n",
    "  random.seed(seed) # manually seed random so that you can get the same random result whenever you run the code for reproducibility\n",
    "  for typ in ('pos', 'neg'):\n",
    "    paths_of_typ = list( (dir_path / typ).glob('*.txt'))\n",
    "    num_examples = len(paths_of_typ)\n",
    "    num_train_sample = num_examples * 4 // 5\n",
    "    \n",
    "    paths_of_typ = sorted(paths_of_typ)\n",
    "    random.shuffle(paths_of_typ) # shuffle the dataset\n",
    "    train_set += paths_of_typ[:num_train_sample] # assign first num_train_sample samples for train set\n",
    "    valid_set += paths_of_typ[num_train_sample:] # assign the remaining samples for validation set\n",
    "    \n",
    "  random.shuffle(train_set)\n",
    "  random.shuffle(valid_set)\n",
    "  \n",
    "  return train_set, valid_set\n",
    "\n",
    "\n",
    "train_pths, valid_pths = get_train_txt_paths_in_split(train_path)\n",
    "test_pths = list(test_path.rglob(\"*.txt\"))\n",
    "\n",
    "print(f\"Number of training data: {len(train_pths)}, validation data: {len(valid_pths)}, test data: {len(test_pths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Print the first 10 paths in train_pths\n",
    "'''\n",
    "train_pths[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acb1ca09",
   "metadata": {},
   "source": [
    "#### Make Vocabulary\n",
    "- This cell makes a vocabulary from the training set\n",
    "  - using ``basic_english`` tokenizer\n",
    "  - using ``Counter`` to count the number of tokens\n",
    "  - using ``min_count`` to remove tokens that appear less than ``min_count`` times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_vocab_from_txt_fns(txt_fns_list, tokenizer):\n",
    "  '''\n",
    "  This function takes a list of txt file paths and returns a list of all the words in the txt files\n",
    "  '''\n",
    "  vocab = Counter()\n",
    "  for txt_fn in txt_fns_list:\n",
    "    with open(txt_fn, 'r') as f:\n",
    "      for line in f:\n",
    "        vocab.update(tokenizer(line))\n",
    "  return vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "entire_vocab = make_vocab_from_txt_fns(train_pths, tokenizer)\n",
    "min_count = 5\n",
    "vocab = sorted([token for token, count in entire_vocab.items() if count >= min_count])\n",
    "\n",
    "print(f\"Number of tokens in the entire vocabulary: {len(entire_vocab)}\")\n",
    "print(f\"Number of tokens in the vocabulary with min_count = {min_count}: {len(vocab)}\")\n",
    "print(f\"First 10 tokens in the vocabulary: {vocab[:10]}\")\n",
    "print(f\"Last 10 tokens in the vocabulary: {vocab[-10:]}\")\n",
    "print(f\"Middle 10 tokens in the vocabulary: {vocab[len(vocab)//2-5:len(vocab)//2+5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8f868",
   "metadata": {},
   "source": [
    "## Problem 1: Complete the dataset class\n",
    "- Complete the given class ``IMDbData`` \n",
    "    - ``IMDbData`` has a list of txt paths. Each txt corresponds to a single data sample.\n",
    "        - **The label, whether the given sentence is positive or negative, is recorded in the name of directory path of the file**\n",
    "        - You can convert ``Path`` instance to ``str`` by ``str(a_path)``\n",
    "    - Complete two special methods ``__len__`` and ``__getitem__``\n",
    "        - ``__len__`` returns the length of the dataset, which is number of total data samples in the dataset\n",
    "        - ``__getitem__`` takes an index and returns the corresponding data sample for a given index\n",
    "        - To read txt file, you can use a pre-defined ``read_txt`` function\n",
    "            - ``read_txt`` gets an txt file path as an input and returns a content of the txt file in a string\n",
    "        - To get a list of token, use ``self.tokenizer``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(txt_path):\n",
    "  with open(txt_path, 'r') as f:\n",
    "    txt_string = f.readline()\n",
    "  return txt_string\n",
    "\n",
    "class IMDbData:\n",
    "  def __init__(self, path_list):\n",
    "    self.paths = path_list\n",
    "    self.tokenizer = get_tokenizer('basic_english')\n",
    "  \n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    __len__ is a special method that returns length of the instance when called with len(class_instance)\n",
    "    e.g.\n",
    "      dataset = IMDbData()\n",
    "      length_of_dataset = len(dataset)\n",
    "      \n",
    "    TODO: Complete this function \n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    __getitem__ is a special method that returns an item for a given index when called with class_instance[index]\n",
    "    e.g.\n",
    "      trainset = IMDbData(train_pths)\n",
    "      trainset[6] == trainset.__getitem__(6)\n",
    "      \n",
    "    output: sequence_of_token, label\n",
    "      sequence_of_token (list): a list of string (word token). Use self.tokenizer to make string into a list of word token\n",
    "      label (int): 0 if the sentence is negative, 1 if the sentence is positive    \n",
    "      \n",
    "    HINT: use str(pth) to convert Path into String.\n",
    "          You can find the label of the sample in its file directory path\n",
    "\n",
    "          \n",
    "    TODO: Complete this function using self.paths, self.tokenizer, and read_txt()\n",
    "    \"\"\"\n",
    "\n",
    "    return \n",
    "\n",
    "trainset = IMDbData(train_pths)\n",
    "validset = IMDbData(valid_pths)\n",
    "short_validset = IMDbData(valid_pths[:100])\n",
    "testset = IMDbData(test_pths)\n",
    "\n",
    "# print('__len__ result for trainset: ', len(trainset))\n",
    "# print('__getitem__ result: ', trainset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Examples of how tokenizer works\n",
    "'''\n",
    "trainset.tokenizer('this is my example Sentence!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test your IMDbData class\n",
    "'''\n",
    "trainset = IMDbData(train_pths)\n",
    "assert len(trainset) == 20000 and len(validset) ==5000 and len(short_validset)==100\n",
    "assert len(trainset[0]) == 2\n",
    "assert trainset[154][0][10:15] == ['ends', 'right', 'after', 'this', 'little'], \"Error in the trainset __getitem__ output\"\n",
    "assert trainset[594][1] == 0 and trainset[523][1] == 1 and trainset[1523][1] == 0, \"Error in the trainset __getitem__ output\"\n",
    "\n",
    "print(\"Passed all the test cases!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a3e235a",
   "metadata": {},
   "source": [
    "## Problem 2: Complete String to idx Converter\n",
    "- Complete a class for converting a list of string to a list of integer\n",
    "    - use the variable ``vocab``, which is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Str2Idx2Str:\n",
    "  def __init__(self, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "      vocab: a list of strings, which contains all the words in the vocabulary\n",
    "    TODO: Complete the class\n",
    "    \n",
    "    1. Declare self.idx2str\n",
    "    - self.idx2str is a list of strings, which contains a string of word that corresponds to the index of the list\n",
    "    - e.g. self.idx2str[your_index] returns a string value of your_index of the vocabulary\n",
    "    - Use the input argument vocab to initialize self.idx2str\n",
    "    \n",
    "    2. Declare self.str2idx\n",
    "    - self.str2idx is a dictionary, where its keys are the words in strings and its values are the corresponding index of each word\n",
    "    - e.g. self.str2idx[your_word] returns an integer value of the index of your_word in the vocabulary\n",
    "\n",
    "    '''\n",
    "    self.idx2str = []\n",
    "    self.str2idx = {}\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    You have to add these lines,\n",
    "    And explain in your report what is the function of these two lines \n",
    "    '''\n",
    "    self.unknown_idx = len(self.str2idx)\n",
    "    self.idx2str.append(\"UNKNOWN\")\n",
    "  \n",
    "    \n",
    "  \n",
    "  def __call__(self, alist):\n",
    "    '''\n",
    "    This function converts list of word string to its index and vice versa.\n",
    "    For example, if it takes ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you'] as an input,\n",
    "    it will return [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81].\n",
    "    \n",
    "    If it takes [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81],\n",
    "    it will return ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
    "    \n",
    "    If it takes a list of list of string, such as [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']],\n",
    "    it will return a list of list of integer, [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]]\n",
    "    \n",
    "    Vice versa, if it takes [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]] as an input,\n",
    "    it will return [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']],\n",
    "    \n",
    "    Input: alist of strings, or a list of integers, or a list of lists\n",
    "      e.g. alist = ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
    "        or alist = [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81]\n",
    "        or alist = [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']]\n",
    "        or alist = [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]]\n",
    "    \n",
    "    IMPORTANT: If a word in the input list is not in the vocabulary of Str2Idx2Str, then it has to convert it into UNKNOWN token.\n",
    "    \n",
    "    \n",
    "    Output: a list of integer\n",
    "    \n",
    "    TODO: Complete this function, using self.idx2str and self.str2idx\n",
    "    \n",
    "    Hint: You can figure out the type of input by using the function isinstance. It will return boolean.\n",
    "        isinstance(an_item, list)\n",
    "        isinstance(an_item, str)\n",
    "        isinstance(an_item, int)\n",
    "    '''\n",
    "    \n",
    "    # Write your code from here\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "# Test the code\n",
    "converter = Str2Idx2Str(vocab)\n",
    "input_sentence = trainset[0][0][:20] #0th sample, text (instead of label), first 20 words\n",
    "print(f\"Input sentence: {input_sentence}\")\n",
    "print(f\"Converted sentence: {converter(input_sentence)}\")\n",
    "print(f\"Re-converted sentence: {converter(converter(input_sentence))}\")\n",
    "print(f\"Result for a list of sentences/ input_list: {[trainset[i][0][:5]for i in range(1,5)]}, output_list: {converter([trainset[i][0][:5]for i in range(1,5)])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edac0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test your code by running this cell.\n",
    "\n",
    "Don't change the test cases\n",
    "'''\n",
    "\n",
    "list_of_string = ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
    "list_of_intger = [11987, 1487, 26426, 14417, 13830, 1310, 11072, 3792, 26166, 24238, 15911, 1310, 9041, 10407, 551, 12881, 76, 26889]\n",
    "\n",
    "assert converter(list_of_string) == list_of_intger, \\\n",
    "    f\"The output of converting list_of_string has to be same with list_of_intger. Your current output is {converter(list_of_string)}\"\n",
    "assert converter(list_of_intger) == list_of_string, \\\n",
    "    f\"The output of converting list_of_intger has to be same with list_of_string. Your current output is {converter(list_of_intger)}\"\n",
    "\n",
    "\n",
    "list_of_string_list = [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']]\n",
    "list_of_integer_list = [[902, 26173, 551, 10912, 16765], [19394, 1092, 16765, 24153, 5020], [26447, 11120, 24238, 16550, 2483], [24238, 12847, 493, 19419, 23088]]\n",
    "\n",
    "assert converter(list_of_string_list) == list_of_integer_list, \\\n",
    "    f\"The output of converting list_of_string_list has to be same with list_of_integer_list. Your current output is {converter(list_of_string_list)}\"\n",
    "assert converter(list_of_integer_list) == list_of_string_list, \\\n",
    "    f\"The output of converting list_of_integer_list has to be same with list_of_string_list. Your current output is {converter(list_of_integer_list)}\"\n",
    "\n",
    "print(\"Passed all the test cases!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0274efa4",
   "metadata": {},
   "source": [
    "## Problem 3: Complete Collate Function\n",
    "- Every data sample in ``IMDbData`` has different length\n",
    "    - Therefore, you have to handle various input length to group the multiple sequnece samples as a tensor\n",
    "- You have to implement ``pack_collate`` which takes a raw batch from the dataset and groups it into a ``PackedSequence``\n",
    "    - You don't need to know about ``PackedSequence`` now. It helps to implement an efficient computation for sequence with different lengths\n",
    "- Implement two variables, following the description in the function\n",
    "    - ``txts_in_idxs``, ``labels``\n",
    "    - use ``self.converter`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85545f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_sequence\n",
    "\n",
    "class PackCollateWithConverter:\n",
    "  def __init__(self, converter):\n",
    "    self.converter = converter\n",
    "  \n",
    "  def __call__(self, batch):\n",
    "    '''\n",
    "    TODO: Declare variables txts_in_idxs and label_tensor, following the description below\n",
    "    Use self.converter to convert txts to txts_in_idxs\n",
    "\n",
    "    word_sentences_in_idxs: A list of torch.LongTensor. Each element in a list is a sequence of integer, and the each integer represents a vocabulary index of word in a sentence.\n",
    "                  i-th element of word_sentences_in_idxs corresponds to the i-th data sample in the batch  \n",
    "    labels: torch.FloatTensor with a shape of [len(batch)]. i-th value of the tensor represents the label of the i-th data sample in the batch (either 0.0 or 1.0)\n",
    "    '''\n",
    "    \n",
    "    # Write your code from here\n",
    "    word_sentences_in_idxs = []\n",
    "    label_tensor = torch.FloatTensor([0])\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Leave the code below as it is\n",
    "    '''\n",
    "    assert isinstance(word_sentences_in_idxs, list), f\"txts_in_idxs has to be a list, not {type(word_sentences_in_idxs)}\"\n",
    "    assert isinstance(word_sentences_in_idxs[0], torch.LongTensor), f\"An elmenet of txts_in_idxs has to be a torch.LongTensor, not {type(word_sentences_in_idxs[0])}\"\n",
    "    assert isinstance(label_tensor, torch.FloatTensor), f\"labels has to be a torch.FloatTensor, not {type(label_tensor)}\"\n",
    "    assert label_tensor[-1] == batch[-1][1], \"i-th element of labels has to be \"\n",
    "    \n",
    "    packed_sequence = pack_sequence(word_sentences_in_idxs, enforce_sorted=False)\n",
    "\n",
    "    return packed_sequence, label_tensor\n",
    "\n",
    "pack_collate = PackCollateWithConverter(converter)\n",
    "# Test the code\n",
    "train_loader = DataLoader(trainset, batch_size=32, collate_fn=pack_collate, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print('A batch looks like this: ', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96e0d1",
   "metadata": {},
   "source": [
    "### Preparation: Define Model\n",
    "- You don't have to change this code, or try to understand how this GRU model works at the current stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "  def __init__(self,vocab_size, hidden_size=128, num_layers=3, ):\n",
    "    super().__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.3, batch_first=True)\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.final_layer = nn.Linear(hidden_size*2, 1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = PackedSequence(self.word_embedding(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
    "    x, hidden = self.gru(x)\n",
    "    pad_x, lens = pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "    max_x = torch.stack([torch.max(pad_x[i, :lens[i]], dim=0)[0] for i in range(len(lens))], dim=0)\n",
    "\n",
    "    # max_x = torch.max(pad_x, dim=0)[0]\n",
    "    pred_logit = self.final_layer(max_x)[:,0]\n",
    "    return torch.sigmoid(pred_logit)\n",
    "\n",
    "# Test the model\n",
    "model = SentimentModel(len(converter.idx2str))\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "out = model(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e45ef",
   "metadata": {},
   "source": [
    "## Problem 4: Implement Binary Cross Entropy Loss\n",
    "- Without using ``torch.nn.BCELoss``\n",
    "    - You can implement it with ``torch.log`` and ``torch.mean`` or ``atensor.mean()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e39022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_cross_entropy_loss(pred, target, eps=1e-8):\n",
    "  '''\n",
    "  pred (torch.FloatTensor): Prediction value for N samples \n",
    "                            Each element in the tensor is the output of torch.sigmoid, and has a value between 0 and 1\n",
    "  target (torch.FloatTensor): Corresponding target value for N samples. \n",
    "                              Each element in the tensor has value of either 0 or 1\n",
    "  eps (float): A small value to avoid log(0) error\n",
    "  \n",
    "  output: Mean of binary cross entropy of N samples\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  '''\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99634eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test your BCE loss function\n",
    "Don't change the test cases\n",
    "'''\n",
    "\n",
    "test_pred_case = torch.Tensor([9.9894e-01, 2.2645e-03, 1.8131e-01, 8.0153e-03, 9.9972e-01, 1.0378e-03,\n",
    "        9.9949e-01, 9.9967e-01, 6.4150e-03, 9.9912e-01, 9.9896e-01, 1.4350e-01,\n",
    "        9.9896e-01, 2.1979e-02, 9.9976e-01, 4.5389e-03, 9.9906e-01, 1.0633e-02,\n",
    "        9.9749e-01, 5.5501e-04, 7.0052e-04, 2.9509e-04, 3.2752e-04, 9.9940e-01,\n",
    "        4.5912e-04, 9.9969e-01, 6.0225e-03, 9.9974e-01, 9.9907e-01, 9.9942e-01,\n",
    "        4.0911e-01, 2.8850e-01])\n",
    "test_target_case = torch.Tensor([1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
    "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
    "\n",
    "your_result = get_binary_cross_entropy_loss(test_pred_case, test_target_case)\n",
    "\n",
    "'''\n",
    "The value can be little different because of epsilon value used for torch.log\n",
    "'''\n",
    "print(f\"BCE Loss by torch.nn.BCELoss is {torch.nn.BCELoss()(test_pred_case, test_target_case)} and your BCE loss is {your_result}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2497890e",
   "metadata": {},
   "source": [
    "### Problem 5: Complete Training Loop\n",
    "- In this problem, you have to implement a Trainer class\n",
    "    - It contains everything you need to train a neural network model\n",
    "    - model, optimizer, loss function, train loader, validation loader, and device (cuda or cpu)\n",
    "- IMPORTANT\n",
    "    - Select proper ``batch_size`` for ``train_loader``, ``validation_loader``, ``test_loader``\n",
    "\n",
    "- Complete ``self._get_accuracy()``\n",
    "  - If the prediction is larger than 0.5, you can regard it as positive sentiment\n",
    "\n",
    "- Complete ``self._get_loss_and_acc_from_single_batch()``\n",
    "    - using ``self._get_accuracy()``\n",
    "\n",
    "- Complete ``_train_by_single_batch``\n",
    "    - using ``self._get_loss_and_acc_from_single_batch()``\n",
    "    - You can test it on the cell below\n",
    "\n",
    "- Complete ``validate``\n",
    "    - Implement it with **preventing gradient calculation** to reduce memory usage\n",
    "        - Use ``with torch.no_grad():`` or ``with torch.inference_mode():``\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_fn = torch.nn.BCELoss()\n",
    "    self.train_loader = train_loader\n",
    "    self.valid_loader = valid_loader\n",
    "    \n",
    "    self.model.to(device)\n",
    "    \n",
    "    self.best_valid_accuracy = 0\n",
    "    self.device = device\n",
    "    \n",
    "    self.training_loss = []\n",
    "    self.training_acc = []\n",
    "    self.validation_loss = []\n",
    "    self.validation_acc = []\n",
    "\n",
    "  def save_model(self, path='imdb_sentiment_model.pt'):\n",
    "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
    "    \n",
    "  def train_by_num_epoch(self, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "      self.model.train()\n",
    "      for batch in tqdm(self.train_loader, leave=False):\n",
    "        loss_value, acc = self._train_by_single_batch(batch)\n",
    "        self.training_loss.append(loss_value)\n",
    "        self.training_acc.append(acc)\n",
    "\n",
    "      self.model.eval()\n",
    "      validation_loss, validation_acc = self.validate()\n",
    "      self.validation_loss.append(validation_loss)\n",
    "      self.validation_acc.append(validation_acc)\n",
    "      \n",
    "      print(f\"Epoch {epoch+1}, Training Loss: {loss_value:.4f}, Training Acc: {acc:.4f}, Validation Loss: {validation_loss:.4f}, Validation Acc: {validation_acc:.4f}\")\n",
    "      if validation_acc > self.best_valid_accuracy:\n",
    "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
    "        self.save_model('imdb_sentiment_model_best.pt')\n",
    "      else:\n",
    "        self.save_model('imdb_sentiment_model_last.pt')\n",
    "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
    "\n",
    "  def _get_accuracy(self, pred, target, threshold=0.5):\n",
    "    '''\n",
    "    This method calculates accuracy for given prediction and target\n",
    "    \n",
    "    input:\n",
    "      pred (torch.Tensor): Prediction value for a given batch\n",
    "      target (torch.Tensor): Target value for a given batch\n",
    "      threshold (float): Threshold value for deciding whether the prediction is positive or negative. Default value is 0.5\n",
    "      \n",
    "    output: \n",
    "      accuracy (float): Mean Accuracy value for every sample in a given batch\n",
    "    \n",
    "    TODO: Complete this method using all the input arguments\n",
    "    '''\n",
    "\n",
    "    return\n",
    "\n",
    "  def _get_loss_and_acc_from_single_batch(self, batch):\n",
    "    '''\n",
    "    This method calculates loss value for a given batch\n",
    "\n",
    "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
    "\n",
    "    You have to use variables below:\n",
    "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
    "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
    "    self.device (str): 'cuda' or 'cpu'\n",
    "    self._get_accuracy (function): function for calculating accuracy for a given prediction and target\n",
    "\n",
    "    output: \n",
    "      loss (torch.Tensor): Mean binary cross entropy value for every sample in the training batch\n",
    "      acc (float): Accuracy for the given batch\n",
    "    # CAUTION! The output loss has to be torch.Tensor that is backwardable, not a float value or numpy array\n",
    "\n",
    "    TODO: Complete this method\n",
    "    '''\n",
    "\n",
    "    return\n",
    "      \n",
    "  def _train_by_single_batch(self, batch):\n",
    "    '''\n",
    "    This method updates self.model's parameter with a given batch\n",
    "    \n",
    "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
    "    \n",
    "    You have to use methods and variables below:\n",
    "\n",
    "    self._get_loss_and_acc_from_single_batch (function): function for calculating loss value for a given batch    \n",
    "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
    "\n",
    "    output: \n",
    "      loss (float): Mean binary cross entropy value for every sample in the training batch\n",
    "      acc (float): Mean accuracy for the given batch\n",
    "    The model's parameters, optimizer's steps has to be updated inside this method\n",
    "\n",
    "    TODO: Complete this method \n",
    "    '''\n",
    "    return\n",
    "\n",
    "\n",
    "    \n",
    "  def validate(self, external_loader=None):\n",
    "    '''\n",
    "    This method calculates accuracy and loss for given data loader.\n",
    "    It can be used for validation step, or to get test set result\n",
    "    \n",
    "    input:\n",
    "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
    "      \n",
    "    \n",
    "    output: \n",
    "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
    "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
    "      \n",
    "    Use these methods:\n",
    "      self._get_loss_and_acc_from_single_batch (function): function for calculating loss value for a given batch\n",
    "    \n",
    "    TODO: Complete this method \n",
    "    CAUTION: During validation, you can make it faster by not calculating gradient\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ### Don't change this part\n",
    "    if external_loader and isinstance(external_loader, DataLoader):\n",
    "      loader = external_loader\n",
    "      print('An arbitrary loader is used instead of Validation loader')\n",
    "    else:\n",
    "      loader = self.valid_loader\n",
    "      \n",
    "    self.model.eval()\n",
    "    \n",
    "    '''\n",
    "    Write your code from here, using loader, self.model, self.loss_fn.\n",
    "    '''\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Don't change this part\n",
    "\"\"\"\n",
    "model = SentimentModel(len(converter.idx2str), hidden_size=128, num_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False)\n",
    "\n",
    "trainer =  Trainer(model, optimizer, get_binary_cross_entropy_loss, train_loader, valid_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a04f7",
   "metadata": {},
   "source": [
    "#### Check the result\n",
    "- Check your implementation works correctly\n",
    "- Don't change the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is to test trainer._train_by_single_batch\n",
    "\n",
    "If your code is implemented correctly, the loss will go down for this specific batch\n",
    "\"\"\"\n",
    "\n",
    "trainer.model.train()\n",
    "train_batch = next(iter(trainer.train_loader)) # get a batch from train_loader\n",
    "\n",
    "loss_track = []\n",
    "for _ in range(10):\n",
    "  loss_value, acc = trainer._train_by_single_batch(train_batch) # test the trainer\n",
    "  loss_track.append(loss_value)\n",
    "\n",
    "assert isinstance(loss_value, float) and loss_value > 0,  \"The return of trainer._train_by_single_batch has to be a single float value that is larger than 0\"\n",
    "print(f\"Loss value for 10 repetition for the same training batch is  {[f'{loss:.4f}' for loss in loss_track]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is to test trainer.validate\n",
    "\"\"\"\n",
    "\n",
    "short_valid_loader = DataLoader(short_validset, batch_size=50, collate_fn=pack_collate)\n",
    "\n",
    "validation_loss, validation_acc = trainer.validate(short_valid_loader)\n",
    "assert isinstance(validation_loss, float) and isinstance(validation_acc, float), \"Both return value of trainer.validate has to be float\"\n",
    "assert validation_loss > 0, \"Validation Loss has to be larger than 1\"\n",
    "assert 0 <= validation_acc <= 1, \"Validation Acc has to be between 0 and 1\"\n",
    "\n",
    "print(f\"Valid loss: {validation_loss}, Accuracy: {validation_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7cd7a",
   "metadata": {},
   "source": [
    "### Train the model with the completed Trainer\n",
    "- In your report, attach the result of following cells and describe the training result and test result\n",
    "    - Plot of training and validation loss/acc\n",
    "    - Result of your model on test set\n",
    "\n",
    "- [Optional] You can modify the code to train the model in different ways\n",
    "    - optimizer, batch_size, model_size, num_epcohs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29853df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train_by_num_epoch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot the result after the training\n",
    "'''\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.subplot(4,1,1)\n",
    "plt.title(\"Training loss\")\n",
    "plt.plot(trainer.training_loss)\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.plot(trainer.training_acc)\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "plt.title(\"Validation loss by epoch\")\n",
    "plt.plot(trainer.validation_loss)\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "plt.title(\"Validation accuracy by epoch\")\n",
    "plt.plot(trainer.validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eec925",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the test result\n",
    "'''\n",
    "\n",
    "test_loss, test_acc = trainer.validate(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "816b4469",
   "metadata": {},
   "source": [
    "### Paste your code to ``Assignment2.py`` file\n",
    "- Copy and paste your completed code to ``Assignment2.py`` file\n",
    "- You can check your code is correct by running the following cell\n",
    "  - In my test, my last print was\n",
    "    - Valid loss: 0.6931650042533875, Accuracy: 0.5\n",
    "    - Epoch 1, Training Loss: 0.6802, Training Acc: 0.6875, Validation Loss: 0.6942, Validation Acc: 0.5100\n",
    "    - Saving the model with best validation accuracy: Epoch 1, Acc: 0.5100 \n",
    "    - Last 5 Training loss: [0.6907180547714233, 0.6797541379928589, 0.7054318189620972, 0.6722485423088074, 0.6801780462265015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 Assignment2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a2ea1",
   "metadata": {},
   "source": [
    "## Problem 6: Analyze the Prediction of the model\n",
    "- In this problem, you have to anlayze the prediction of the model\n",
    "    - You can select among the two models\n",
    "        - Trainer is designed to save two models\n",
    "        - ``imdb_sentiment_model_last.pt`` contains the model weights after the last training epoch\n",
    "        - ``imdb_sentiment_model_best.pt`` contains the model weights after the training epoch with the best validation accuracy\n",
    "     \n",
    "- If you failed to train your model by solving the previous problems, you can download the model\n",
    "- In your report, describe your analysis on how the trained model works on the text\n",
    "    - What is the main criteria for model to decide whether the review is positive or negative?\n",
    "    - When does it make mistakes? When does it make nice predictions? \n",
    "    - Does the converted input text has enough information to classify text compared to the original text?\n",
    "        - Do you see any problems in tokenizing or using UNKNOWN?\n",
    "- You can write the analysis by using only the Test Set samples, or using your own review texts\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75c333",
   "metadata": {},
   "source": [
    "#### Download Model (if you have failed to trained your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22533d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you run this code, you can download the pretrained weight, named\n",
    "\"imdb_sentiment_model_best_pretrained.pt\", and\n",
    "\"imdb_sentiment_model_last_pretrained.pt\"\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeaf292",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentModel(len(converter.idx2str), 128, 3) # You have to use the same model architecture as the one you used for training\n",
    "your_model_pt_path = 'imdb_sentiment_model_last.pt' # or imdb_sentiment_model_best.pt, or imdb_sentiment_model_last_pretrained.pt etc\n",
    "model.load_state_dict(torch.load(your_model_pt_path, map_location='cpu')['model']) # Or imdb_sentiment_model_best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff2d4d",
   "metadata": {},
   "source": [
    "#### Check the largest error cases from the Test Set\n",
    "- Following code will print out the error case with the largest errors\n",
    "    - Or data sample with smallest error, with ``largest_loss=False``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02737842",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = '\\n'\n",
    "def sort_data_idx_by_loss(model, data_loader, device='cuda'):\n",
    "  assert isinstance(data_loader.sampler, torch.utils.data.sampler.SequentialSampler)\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  entire_loss = []\n",
    "  entire_pred = []\n",
    "  loss_fn = torch.nn.BCELoss(reduction='none')\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "      x, y =batch\n",
    "      pred = model(x.to(device))\n",
    "      loss = loss_fn(pred, y.to(device))\n",
    "      entire_loss += loss.tolist()\n",
    "      entire_pred += pred.tolist()\n",
    "  sorted_indices = sorted(range(len(entire_loss)),key=entire_loss.__getitem__)\n",
    "  return sorted_indices, entire_loss, entire_pred\n",
    "\n",
    "def print_top_k_loss_case(data_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=True):\n",
    "  if descending:\n",
    "    sorted_indices = reversed(sorted_indices[-k:])\n",
    "  else:\n",
    "    sorted_indices = sorted_indices[:k]\n",
    "  \n",
    "  texts = []\n",
    "  for i, idx in enumerate(sorted_indices):\n",
    "    data_sample = data_loader.dataset[idx]\n",
    "    conv_text = ' '.join(converter(converter(data_sample[0])))\n",
    "    orig_text = read_txt(data_loader.dataset.paths[idx])\n",
    "    texts.append({'converted': conv_text, 'original': orig_text})\n",
    "    print(f\" {i}. Sample index: {idx} - Loss: {entire_loss[idx]:.4f}, Model Prediction: {entire_pred[idx]:.4f}, Correct Label: {data_sample[1]} \\\n",
    "          {nl}  Converted Text: {conv_text} {nl}  Original Text: {orig_text} {nl}\")\n",
    "  return texts\n",
    "\n",
    "\n",
    "'''\n",
    "This will calculate loss for each sample in the test set\n",
    "'''\n",
    "sorted_indices, entire_loss, entire_pred =  sort_data_idx_by_loss(model, test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will print out top-k most incorrect prediction on test set\n",
    "'''\n",
    "texts = print_top_k_loss_case(test_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will print out top-k most correct prediction on test set\n",
    "'''\n",
    "\n",
    "texts = print_top_k_loss_case(test_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c078561",
   "metadata": {},
   "source": [
    "#### Test wit your own text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sentiment_of_given_txt(model, input_text):\n",
    "  model.cpu()\n",
    "  model.eval()\n",
    "  tokenizer = trainset.tokenizer\n",
    "  your_text_in_token = tokenizer(input_text)\n",
    "  model_input = pack_sequence([torch.tensor(converter(your_text_in_token), dtype=torch.long)])\n",
    "  prediction = model(model_input).squeeze()\n",
    "  \n",
    "  return prediction\n",
    "\n",
    "\n",
    "your_text = \"\"\"\n",
    "    This movie is terrific\n",
    "\"\"\"\n",
    "\n",
    "estimate_sentiment_of_given_txt(model, your_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
