{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38583105",
      "metadata": {
        "id": "38583105"
      },
      "source": [
        "# Assignment 4: Attention\n",
        "- In this assignment, you have to implement the attention mechanism for the machine translation task.\n",
        "\n",
        "- Problems\n",
        "  - 1. Implement Dot Product Attention (10 pts)\n",
        "  - 2. Implement Attention in Batch (12 pts)\n",
        "  - 3. Implement Seq2seq with Attention (14 pts)\n",
        "  - 4. Implement Transformer-like Attention (14 pts)\n",
        "    - Self-attention\n",
        "    - Cross-attention\n",
        "\n",
        "- Submission\n",
        "  - You have to copy and paste your answer to ``Assignment4.py`` file and submit it to Cyber Campus\n",
        "    - Submssion file name has to be ``Assignment4_{your_student_id}.py``\n",
        "\n",
        "- CAUTION:\n",
        "  - You have to implement most of the functions with **vectorized matrix multiplication**, not with for loop\n",
        "\n",
        "\n",
        "\n",
        "- If you find any error, please do not hesitate to report or make a question on Cyber Campus\n",
        "    - Don't waste too much time on solving the error. The code is not thoroughly checked, and the error can be not your fault."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095f3a9c",
      "metadata": {
        "id": "095f3a9c"
      },
      "outputs": [],
      "source": [
        "# If you are in Colab, install transformers\n",
        "!pip -q install transformers\n",
        "!pip install koreanize-matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ace9b0",
      "metadata": {
        "id": "40ace9b0"
      },
      "outputs": [],
      "source": [
        "# Download the py file for submission\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020-2023/main/Assignment4.py\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020-2023/main/assignment4_pre_defined.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8635fb79",
      "metadata": {
        "id": "8635fb79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import koreanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "import os\n",
        "\n",
        "# Below helps to run tokenizer with multiprocessing\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd6024f",
      "metadata": {
        "id": "ecd6024f"
      },
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f182b757",
      "metadata": {
        "id": "f182b757",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This is the example of vectorization of dot product for two different sequence length.\n",
        "'''\n",
        "\n",
        "e_states = torch.randn(100, 16)\n",
        "d_states = torch.randn(80, 16)\n",
        "\n",
        "dot_product = torch.mm(e_states, d_states.permute(1,0)) # (100, 16) x (16, 80) = (100, 80)\n",
        "dot_product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfefeae",
      "metadata": {
        "id": "fcfefeae"
      },
      "source": [
        "## Problem 1: Implement Dot Product Attention\n",
        "\n",
        "- Optimizing computation time is really important\n",
        "    - Use `torch.mm()` or `torch.matmul()`\n",
        "    - `torch.mm(a, b)` is a function for calculating matrix multiplcation of two matrices `a` and `b`\n",
        "        - `a` and `b` has to be 2-dim tensors\n",
        "        - `a.shape[1]` has to be equal to `b.shape[0]`\n",
        "    - `torch.matmul()` is a function for matrix multiplication but with broadcasting\n",
        "        - https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "        - It has less restriction on its input shape.\n",
        "            - It automatically matches the dimension of two tensors following some rules\n",
        "            - Therefore, it is a bit risky to use this funciton if you don't understand how it works\n",
        "- **DO NOT** use element-wise product or for loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2059e903",
      "metadata": {
        "id": "2059e903"
      },
      "source": [
        "### Hint: Dot product as matrix multiplcation.\n",
        "\n",
        "- Let's say there are two vector, $u=\\begin{bmatrix}-3 \\\\ 2 \\\\ 1\\end{bmatrix}$ and $v = \\begin{bmatrix} 5 \\\\ 4 \\\\ 6\\end{bmatrix}$\n",
        "    - The dot product of the two vectors is $(-3 \\times 5) + (2 \\times 4) + (1 \\times 6) = 1$\n",
        "    - It is equivalent to $u^T \\times v$\n",
        "        - In this case $u\\in\\mathbb{R}^{3\\times1}$ and $v\\in\\mathbb{R}^{3\\times1}$\n",
        "- In PyTorch, this can be described as below:\n",
        "    - `u = torch.Tensor([-3, 2, 1])`\n",
        "    - `v = torch.Tensor([5, 4, 6])`\n",
        "    - Dot product of u and v can be calculated by one of belows:\n",
        "        - `torch.mm(u.unsqueeze(0), v.unsqueeze(1))`\n",
        "            - `u.unsqueeze(0).shape == [1, 3]`\n",
        "            - `v.unsqueeze(1).shape == [3, 1]`\n",
        "            - `unsqueeze()` returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "            - The result has shape of [1,1]\n",
        "        - `torch.matmul(u, v)`\n",
        "        - `u @ v`\n",
        "            - `@` denotes matrix multiplication, which was introduced from Python 3.5\n",
        "        - `(u * v).sum()`\n",
        "            - This will be much slower than others, because it first do element-wise multiplcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4aed8a2",
      "metadata": {
        "id": "a4aed8a2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hint: Dot product as matrix multiplcation.\n",
        "'''\n",
        "\n",
        "u = torch.Tensor([-3, 2, 1])\n",
        "v = torch.Tensor([5, 4, 6])\n",
        "\n",
        "print(f\"Result of (u * v).sum() is {(u * v).sum()}. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\")\n",
        "print(f\"Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is {torch.mm(u.unsqueeze(0), v.unsqueeze(1))}\")\n",
        "print(f\"Result of torch.matmul(u, v) is {torch.matmul(u, v)}\")\n",
        "print(f\"Result of u @ v is {u @ v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15156e7",
      "metadata": {
        "id": "b15156e7"
      },
      "source": [
        "### 1-1 Get attention score with dot product (4 pts)\n",
        "- From a sequence of key and a single query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a20b45",
      "metadata": {
        "id": "66a20b45"
      },
      "outputs": [],
      "source": [
        "def get_attention_score_for_a_single_query(keys, query):\n",
        "  '''\n",
        "  This function returns an attention score for each vector in keys for a given query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [T]\n",
        "\n",
        "    attention_score[i] has to be a dot product value between keys[i] and query\n",
        "\n",
        "\n",
        "  TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
        "  Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_t, h_size)\n",
        "query = torch.randn(h_size)\n",
        "\n",
        "att_score = get_attention_score_for_a_single_query(keys, query)\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce66bf6c",
      "metadata": {
        "id": "ce66bf6c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case\n",
        "'''\n",
        "assert att_score.ndim == 1 and len(att_score) == num_t, \"Error: Check output shape\"\n",
        "answer = torch.Tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
        "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
        "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])\n",
        "assert torch.allclose(att_score, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e9cd39",
      "metadata": {
        "id": "97e9cd39"
      },
      "source": [
        "### 1-2 Get attention weight from score (2pts)\n",
        "- Convert attention score to attention weight\n",
        "- Use softmax function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63d332d",
      "metadata": {
        "id": "b63d332d"
      },
      "outputs": [],
      "source": [
        "def get_attention_weight_from_score(attention_score):\n",
        "  '''\n",
        "  This function converts attention score to attention weight.\n",
        "\n",
        "  Argument:\n",
        "    attention_score (torch.Tensor): Tensor of real number. Has a shape of [T]\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): Tensor of real number between 0 and 1. Sum of attention_weight is 1. Has a shape of [T]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  assert attention_score.ndim == 1\n",
        "\n",
        "  return\n",
        "\n",
        "att_weight = get_attention_weight_from_score(att_score)\n",
        "att_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8030b21a",
      "metadata": {
        "id": "8030b21a"
      },
      "outputs": [],
      "source": [
        "answer = torch.Tensor([0.0000,     0.0013,     0.0009,     0.0000,     0.0041,     0.0002,\n",
        "            0.0001,     0.0000,     0.0002,     0.0028,     0.0001,     0.0001,\n",
        "            0.0002,     0.0001,     0.0004,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0022,     0.0001,     0.9756,     0.0118])\n",
        "assert att_weight.shape == att_score.shape, 'Shape has to be remained the same'\n",
        "assert att_weight.sum() == 1, \"Sum of attention weight has to be 1\"\n",
        "assert torch.allclose(att_weight, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e2309e",
      "metadata": {
        "id": "26e2309e"
      },
      "source": [
        "### 1-3 Get weighted sum (4 pts)\n",
        "- Using attention weight and values, get the weighted sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf8e701",
      "metadata": {
        "id": "6bf8e701"
      },
      "outputs": [],
      "source": [
        "def get_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [T, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [T], which represents the weight for each vector to compose the attention vector\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
        "\n",
        "  TODO: Complete this function using torch.mm\n",
        "  '''\n",
        "  return\n",
        "\n",
        "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
        "att_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1253844f",
      "metadata": {
        "id": "1253844f"
      },
      "outputs": [],
      "source": [
        "answer = torch.Tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
        "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])\n",
        "assert att_vec.shape == query.shape, 'Shape has to be remained the same'\n",
        "assert torch.allclose(att_vec, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc00df77",
      "metadata": {
        "id": "cc00df77"
      },
      "source": [
        "## Problem 2: Attention in Batch ( 16 pts)\n",
        "- In this problem, you have to calculate attention with batch\n",
        "- You can use `torch.bmm()` for batch matrix multiplication https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "    - `torch.bmm()` takes two 3-dim tensor as its input\n",
        "    - Each tensor has to be 3-dim (atensor.ndim==3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062446fe",
      "metadata": {
        "id": "062446fe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hint for Problem 2\n",
        "\n",
        "You can calculate matrix multiplication of matrices in batch effectively using torch.bmm() or torch.matmul()\n",
        "'''\n",
        "\n",
        "torch.manual_seed(0)\n",
        "matrix_left1 = torch.randn(5, 3)\n",
        "matrix_left2 = torch.randn(5, 3)\n",
        "\n",
        "print(f\"matrix_left1: \\n{matrix_left1}\")\n",
        "print(f\"matrix_left2: \\n{matrix_left2}\")\n",
        "\n",
        "matrix_right1 = torch.randn(3, 4)\n",
        "matrix_right2 = torch.randn(3, 4)\n",
        "print(f\"matrix_right1: \\n{matrix_right1}\")\n",
        "print(f\"matrix_right2: \\n{matrix_right2}\")\n",
        "\n",
        "print(\"Let's assume that we have batch of matrix, which is stack of these two matices\")\n",
        "matrix_left = torch.stack([matrix_left1, matrix_left2])\n",
        "matrix_right = torch.stack([matrix_right1, matrix_right2])\n",
        "\n",
        "print(f\"matrix_left: \\n{matrix_left} \\n which is shape of {matrix_left.shape}\")\n",
        "print(f\"matrix_right: \\n{matrix_right}\\n which is shape of {matrix_right.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Exhaustive method: using torch.mm() only with for loop (This is SLOW when matrix gets much larger)\n",
        "'''\n",
        "\n",
        "mm_forloop_output = []\n",
        "for sample_index in range(matrix_left.shape[0]):\n",
        "  mat_left = matrix_left[sample_index]\n",
        "  mat_right = matrix_right[sample_index]\n",
        "\n",
        "  mm_result = torch.mm(mat_left, mat_right)\n",
        "  mm_forloop_output.append(mm_result)\n",
        "\n",
        "mm_forloop_stack = torch.stack(mm_forloop_output)\n",
        "print(f\"mat_mul_stack: \\n{mm_forloop_stack}\\n which is shape of {mm_forloop_stack.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Good method: using torch.bmm()\n",
        "'''\n",
        "\n",
        "mat_mul_bmm = torch.bmm(matrix_left, matrix_right)\n",
        "print(f\"mat_mul_bmm: \\n{mat_mul_bmm}\\n which is shape of {mat_mul_bmm.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd08038",
      "metadata": {
        "id": "9bd08038"
      },
      "source": [
        "### 2-1 Get attention score in batch (4 pts)\n",
        "- Now keys and query exist in batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe689641",
      "metadata": {
        "id": "fe689641"
      },
      "outputs": [],
      "source": [
        "def get_attention_score_for_a_batch_query(keys, query):\n",
        "  '''\n",
        "  This function returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [N, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, T]\n",
        "\n",
        "    attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
        "\n",
        "  '''\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b,num_t, h_size)\n",
        "query = torch.randn(num_b, h_size)\n",
        "out = get_attention_score_for_a_batch_query(keys, query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59da18f0",
      "metadata": {
        "id": "59da18f0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([ -2.7744,   1.3793,   5.0969,   0.7559,   2.5898,  -0.9475,  -1.1960,\n",
        "           5.4975,   0.4018,   5.9949,  -5.9428,  -0.4441,   0.6729,  -0.8326,\n",
        "           3.7091,   1.4913,   2.2062,  -0.2244,  -4.0612,   2.9037,  10.6111,\n",
        "           4.1383,  -4.6549])\n",
        "\n",
        "assert out.ndim == 2 and out.shape == torch.Size([num_b, num_t]), \"Error: Check output shape\"\n",
        "assert torch.allclose(out[2], answer, atol=1e-4), \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a24922",
      "metadata": {
        "id": "25a24922"
      },
      "source": [
        "### 2-2 Get attention score in batch (4 pts)\n",
        "- Implement the same function but in batchified queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae30995b",
      "metadata": {
        "id": "ae30995b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
        "  '''\n",
        "  Now you have to implement the attention score for not only single query, but multiple queries.\n",
        "\n",
        "  This function returns a batch of attention score for each vector in keys for given queries.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that a query wants attend to\n",
        "    queries (torch.Tensor): Has a shape of [N, Tt, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "\n",
        "    attention_score[n, i, t] has to be a dot product value between keys[n, i] and query[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
        "        Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
        "        and how the result tensor has to look like (shape)\n",
        "  '''\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_ts = 23\n",
        "num_tt = 14\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b, num_ts, h_size)\n",
        "queries = torch.randn(num_b, num_tt, h_size)\n",
        "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb45f70",
      "metadata": {
        "id": "1cb45f70"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
        "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
        "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
        "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
        "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
        "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_ts, num_tt]), 'Check the output shape'\n",
        "assert torch.allclose(att_score[2,4], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_score[3,:,2], answer2, atol=1e-4),  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ab274c",
      "metadata": {
        "id": "51ab274c"
      },
      "source": [
        "### 2-3 Get Masked Softmax (4 pts)\n",
        "- Implement masked softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ef2611",
      "metadata": {
        "id": "19ef2611",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def get_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "\n",
        "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: You can give -infinity value by -float(\"inf\")\n",
        "\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "mask = torch.ones_like(att_score)[..., 0]\n",
        "mask[4, 15:] = 0\n",
        "mask[5, 17:] = 0\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[4, 15:] = 0\n",
        "attention_weight = get_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99abd97a",
      "metadata": {
        "id": "99abd97a"
      },
      "outputs": [],
      "source": [
        "answer = torch.Tensor([0.0120,     0.0002,     0.0901,     0.0003,     0.0259,     0.0036,\n",
        "            0.5617,     0.0108,     0.2508,     0.0054,     0.0001,     0.0010,\n",
        "            0.0000,     0.0005,     0.0375,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000])\n",
        "assert torch.allclose(attention_weight[4,:,3], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(attention_weight.sum(1),  torch.tensor([1.0]) , atol=1e-6 ), 'Sum of attention weight has to be 1'\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified), \"Output is different even though only masked part is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2030f5",
      "metadata": {
        "id": "ca2030f5"
      },
      "source": [
        "### 2-3 Implement weighted sum in batchified version (4 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2e5c36",
      "metadata": {
        "id": "9b2e5c36",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def get_batch_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [N, Ts, Tt], which represents the weight for each vector to compose the attention vector\n",
        "                      attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight.\n",
        "                                     Has a shape of [N, Tt, C]\n",
        "\n",
        "  TODO: Complete this function using torch.bmm\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
        "att_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93231ea",
      "metadata": {
        "id": "c93231ea"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.9348, -1.2628, -0.9189, -0.3434, -1.6476,  0.1031, -0.6963, -0.7462,\n",
        "         0.1484,  0.6810,  0.7950,  1.0277, -1.5988,  0.4232, -1.5540,  0.1801])\n",
        "answer2 = torch.Tensor([-0.9204, -0.9710,  0.3062, -1.0122,  1.1933,  0.1302, -1.0280,  0.0095,\n",
        "         0.6124,  0.0615, -1.2312, -0.6714, -0.1764, -0.1254])\n",
        "assert att_out.ndim == 3 and att_out.shape == torch.Size([num_b, num_tt, h_size]), 'Check the output shape'\n",
        "assert torch.max(torch.abs(att_out[2, 5] - answer)) < 1e-4, 'Calculated result is wrong'\n",
        "assert torch.max(torch.abs(att_out[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebbdbf2b",
      "metadata": {
        "id": "ebbdbf2b"
      },
      "source": [
        "## Problem 3: Make seq2seq with attention (14 pts)\n",
        "- Using Pre-defined `TranslatorBi` class, complete a new `TranslatorAtt` class\n",
        "- If you implement it correctly, you can translate a sentence and get the corresponding attention map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904d44a3",
      "metadata": {
        "id": "904d44a3"
      },
      "source": [
        "### 3-0 Prepare dataset and tokenizer\n",
        "- To use the pretrained model correctly, you can use the pretrained vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c670709",
      "metadata": {
        "id": "9c670709"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
        "!unzip -q nia_korean_english_csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4edd48e",
      "metadata": {
        "id": "d4edd48e"
      },
      "outputs": [],
      "source": [
        "# Load data and tokenizer\n",
        "\n",
        "df = pd.read_csv('nia_korean_english.csv')\n",
        "\n",
        "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n",
        "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8fa529",
      "metadata": {
        "id": "ab8fa529"
      },
      "outputs": [],
      "source": [
        "class TranslationSet:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "\n",
        "entireset = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
        "trainset, validset, testset = torch.utils.data.random_split(entireset, [int(len(entireset)*0.9), int(len(entireset)*0.05), len(entireset)-int(len(entireset)*0.9)-int(len(entireset)*0.05)], generator=torch.Generator().manual_seed(42))\n",
        "# trainset, validset, testset = torch.utils.data.random_split(entireset, [360000, 20000, 20000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f'Dataset Item Example: {entireset[0]}')\n",
        "print(f'Length of split : Train {len(trainset)}, Valid {len(validset)}, Test {len(testset)}')\n",
        "\n",
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a254bfb6",
      "metadata": {
        "id": "a254bfb6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "'''\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "\n",
        "    self.model.to(device)\n",
        "\n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "\n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_attention_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "\n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      with tqdm(self.train_loader, leave=False) as pbar:\n",
        "        for batch in pbar:\n",
        "          loss_value = self._train_by_single_batch(batch)\n",
        "          self.training_loss.append(loss_value)\n",
        "          pbar.set_description(f\"Epoch {epoch+1}, Loss {loss_value:.4f}\")\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "\n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_attention_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_attention_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    prob = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(prob, PackedSequence):\n",
        "      loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    '''\n",
        "\n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt, shifted_tgt = batch\n",
        "        src = src.to(self.device)\n",
        "        tgt = tgt.to(self.device)\n",
        "        shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "        prob = self.model(src, tgt)\n",
        "\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(prob, shifted_tgt)\n",
        "\n",
        "        validation_loss += loss.item() * len(prob.data)\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          num_correct_guess += (prob.data.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (prob.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        num_data += len(prob.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "\n",
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  '''\n",
        "  for PackedSequence, the input is 2D tensor\n",
        "\n",
        "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
        "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
        "  '''\n",
        "\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a011e3",
      "metadata": {
        "id": "11a011e3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "\n",
        "You don't need to change this code\n",
        "'''\n",
        "class TranslatorBi(nn.Module):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
        "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
        "\n",
        "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
        "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
        "\n",
        "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
        "\n",
        "  def run_encoder(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb_x = self.src_embedder(x)\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
        "\n",
        "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
        "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
        "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
        "    if isinstance(x, PackedSequence):\n",
        "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
        "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
        "    else:\n",
        "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
        "\n",
        "\n",
        "    return enc_hidden_state_by_t, last_hidden_sum\n",
        "\n",
        "  def run_decoder(self, y, last_hidden_state):\n",
        "    if isinstance(y, PackedSequence):\n",
        "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      emb_y = self.tgt_embedder(y)\n",
        "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
        "    return out, decoder_last_hidden\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    '''\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if isinstance(out, PackedSequence):\n",
        "      logits = self.decoder_proj(out.data)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      logits = self.decoder_proj(out)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2060d164",
      "metadata": {
        "id": "2060d164"
      },
      "source": [
        "### Problem 3.1: Complete the Seq2Seq with Attention (8 pts)\n",
        "- **Caution**: You have to concatenate [decoder_hidden_state; attention_out] for this implementation\n",
        "    - You can use different order of concatenation, but the pre-trained model used that specific order, so please follow it so that you can use the pre-trained weight correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e285f77",
      "metadata": {
        "id": "9e285f77",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class TranslatorAtt(TranslatorBi):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
        "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
        "\n",
        "    # define new self.decoder_proj\n",
        "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
        "\n",
        "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
        "    '''\n",
        "    Arguments:\n",
        "      encoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of encoder GRU. Shape: [N, Ts, C]\n",
        "      decoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of decoder GRU. Shape: [N, Tt, C]\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ts]\n",
        "\n",
        "    Outputs:\n",
        "      attention_vectors (torch.Tensor or PackedSequence): Attention vectors that has the same shape as decoder_hidden_states\n",
        "      attention_weights (torch.Tensor): Zero-padded attention weights.\n",
        "                                You don't need to return it during the training, but it will help you to implement later problem\n",
        "\n",
        "    TODO: Complete this function using following functions\n",
        "      get_attention_score_for_a_batch_multiple_query\n",
        "      get_masked_softmax\n",
        "      get_batch_weighted_sum\n",
        "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
        "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True) to convert PackedSequence to Tensor\n",
        "    Use torch.nn.utils.rnn.pack_padded_sequence(tensor, batch_lens, batch_first=True) to convert Tensor to PackedSequence\n",
        "    '''\n",
        "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
        "    if is_packed:\n",
        "      encoder_hidden_states, source_lens = pad_packed_sequence(encoder_hidden_states, batch_first=True)\n",
        "      decoder_hidden_states, target_lens = pad_packed_sequence(decoder_hidden_states, batch_first=True)\n",
        "\n",
        "    # Write your code from here\n",
        "\n",
        "    # 1. Calculate attention score using encoder_hidden_states and decoder_hidden_states\n",
        "    # 2. Mask the attention score using mask and apply softmax to get attention weight\n",
        "    # 3. Calculate attention vector using attention weight and encoder_hidden_states\n",
        "\n",
        "\n",
        "    #\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    Arguments:\n",
        "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    Output:\n",
        "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    is_packed = isinstance(x, PackedSequence)\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if is_packed:\n",
        "      mask = pad_packed_sequence(x, batch_first=True)[0] != 0\n",
        "    else:\n",
        "      mask = torch.ones(x.shape[0], x.shape[1])\n",
        "\n",
        "    attention_vec, attention_weight = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
        "\n",
        "    # TODO: Write your code from here\n",
        "    # CAUTION:\n",
        "    #   For the concatenation, you have to concat [dec_hidden_state_by_t; attention_vec], not [attention_vec; dec_hidden_state_by_t]\n",
        "    return\n",
        "\n",
        "\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
        "\n",
        "model(batch[0], batch[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3212344b",
      "metadata": {
        "id": "3212344b"
      },
      "source": [
        "#### Test your model\n",
        "- To evaluate your implementation, you have to load the pretrained weight of the same model.\n",
        "- If your implementation is correct, the resulting value would be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4cba9e",
      "metadata": {
        "id": "2d4cba9e"
      },
      "outputs": [],
      "source": [
        "# Download pre-trained model and tensor data\n",
        "!gdown 1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4\n",
        "!gdown 1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b291ec1",
      "metadata": {
        "id": "3b291ec1"
      },
      "outputs": [],
      "source": [
        "# Load pretrained weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.eval()\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Load the pre-calculated example and result\n",
        "prob3_values = torch.load('assignment4_values.pt')\n",
        "single_batch_example, packed_batch_example, correct_single_out, correct_packed_out = prob3_values['single_test_batch'], prob3_values['packed_test_batch'], prob3_values['correct_single_out'],  prob3_values['correct_packed_out']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6051c11f",
      "metadata": {
        "id": "6051c11f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case for Single-size Batch\n",
        "'''\n",
        "single_out = model(single_batch_example[0], single_batch_example[1])\n",
        "\n",
        "assert isinstance(single_out, torch.Tensor), \"The output of model for Tensor has to be Tensor\"\n",
        "assert torch.allclose(single_out, correct_single_out, atol=1e-4), \"The output value is different from the expected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8027b42c",
      "metadata": {
        "id": "8027b42c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case for Batch with PackedSequence\n",
        "'''\n",
        "packed_out = model(packed_batch_example[0], packed_batch_example[1])\n",
        "\n",
        "assert isinstance(packed_out, PackedSequence), \"The output of model for PackedSequence has to be PackedSequence\"\n",
        "assert (packed_out.batch_sizes == correct_packed_out.batch_sizes).all(), \"Output's batch_sizes is wrong\"\n",
        "assert (packed_out.sorted_indices == correct_packed_out.sorted_indices).all(), \"Output's sorted_indices is wrong\"\n",
        "\n",
        "assert torch.allclose(packed_out.data, correct_packed_out.data, atol=1e-4),  \"The output value is different from the expected\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e0ab23",
      "metadata": {
        "id": "12e0ab23"
      },
      "source": [
        "### Train the model (Optional)\n",
        "- You can try to train your model, but you can just load the pretrained data\n",
        "- You don't have to train the model yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d4bf8b",
      "metadata": {
        "id": "75d4bf8b"
      },
      "outputs": [],
      "source": [
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "trainer = Trainer(model, optimizer, nll_loss, train_loader, valid_loader, 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Otherwise, you have to load the model weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "najCuqk2WIbQ"
      },
      "id": "najCuqk2WIbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cea22364",
      "metadata": {
        "id": "cea22364"
      },
      "source": [
        "### Problem 3.2: Implement Inference with Attention Weights (6 pts)\n",
        "- In this problem, you have to implement an inference code that returns translation for given source sentence, but also **attention weights** between source sentence and target sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef0e100",
      "metadata": {
        "id": "1ef0e100"
      },
      "outputs": [],
      "source": [
        "def translate(model, source_sentence):\n",
        "  '''\n",
        "  This function translates a given sentence using a given model.\n",
        "  It returns the tokenized source sentence, tokenized translated sentence, translated sentence in string, and attention map\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "    attention_map (torch.Tensor): Attention weight between each token of source sentence and target sentence. Has a shape of [Ts, Tt]\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = model.src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  mask = torch.ones_like(input_tensor)\n",
        "  enc_hidden_state_by_t, last_hidden_sum = model.run_encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_hidden = last_hidden_sum\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "  total_output = []\n",
        "  total_attetion_weights = []\n",
        "\n",
        "  for i in range(100): # You can chage it to while True:\n",
        "    emb = model.tgt_embedder(current_decoder_token)\n",
        "    '''\n",
        "    TODO: Complete the code here\n",
        "\n",
        "    You have to\n",
        "      1) run decoder rnn for a single step\n",
        "      2) get attention weight (variable name: att_weight) and attention vector.\n",
        "         att_weight.shape == torch.Size([1, len(tokenized_sentence), 1])\n",
        "      3) concat decoder out and attention vector\n",
        "      4) calculate probabilty logit (variable name: logit)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # You don't have to change the codes below.\n",
        "    # Declare logit and last_hidden properly so that the code below can run without error\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = selected_token\n",
        "    current_hidden = last_hidden\n",
        "    if current_decoder_token == 3: ## end of sentence token\n",
        "      break\n",
        "    total_output.append(selected_token[0])\n",
        "    total_attetion_weights.append(att_weight[0,:,0])\n",
        "  predicted_tokens = torch.cat(total_output, dim=0).tolist()\n",
        "  attention_map = torch.stack(total_attetion_weights, dim=1)\n",
        "\n",
        "  return  input_tokens, predicted_tokens, model.tgt_tokenizer.decode(predicted_tokens), attention_map\n",
        "\n",
        "model.cpu()\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "print(translated_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491f62ca",
      "metadata": {
        "id": "491f62ca"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case\n",
        "'''\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "\n",
        "correct_output = 'using this algorithm, you can see how korean words and english words are connected.'\n",
        "answer = torch.tensor([1.6955e-07, 1.1118e-07, 6.0198e-10, 9.1661e-01, 7.8413e-18, 1.2012e-17,\n",
        "        7.6764e-29, 1.2549e-20, 5.7431e-19, 3.3563e-31, 1.1225e-21, 1.3192e-27,\n",
        "        3.6772e-26, 2.2244e-23, 3.6098e-20, 1.5309e-21, 7.5093e-05])\n",
        "answer2 = torch.tensor([1.1643e-11, 3.9906e-30, 1.2813e-33, 2.7519e-13, 2.3483e-15, 4.2758e-12,\n",
        "        1.9385e-18, 6.8541e-16, 4.9662e-18, 5.0304e-33, 7.2299e-26, 4.4580e-25,\n",
        "        3.7096e-23, 7.5614e-22, 4.5226e-22, 2.3576e-25, 1.7577e-12])\n",
        "answer3 = torch.tensor([1.2012e-17, 1.7528e-21, 1.1316e-18, 2.7204e-17, 8.2384e-10, 3.4510e-11,\n",
        "        1.8289e-09, 1.1806e-12, 3.9218e-19, 2.8321e-16, 1.3933e-12, 3.6876e-10,\n",
        "        4.1782e-07, 1.0905e-02, 9.8673e-01, 2.3651e-03, 1.7207e-08, 4.2758e-12])\n",
        "\n",
        "\n",
        "assert translated_string == correct_output, 'Translated sentence is wrong'\n",
        "assert att_weights.shape == torch.Size([18, 17]), 'Attention weight has wrong shape'\n",
        "assert torch.allclose(att_weights[0], answer, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[-1], answer2, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[:,5], answer3, rtol=1e-4), 'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e466b4f4",
      "metadata": {
        "id": "e466b4f4"
      },
      "source": [
        "### Plot attention map\n",
        "- If you completed `translate()`, you can visualize the result of attention weight as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bf81f6",
      "metadata": {
        "id": "f1bf81f6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(len(pred_tokens)*0.8, len(input_tokens)*0.8))\n",
        "x_axis_label = [model.tgt_tokenizer.decode(x) for x in pred_tokens]\n",
        "y_axis_label = [model.src_tokenizer.decode(x) for x in input_tokens]\n",
        "\n",
        "plt.imshow(att_weights.detach())\n",
        "plt.xticks(range(len(x_axis_label)), x_axis_label, fontsize=15,rotation = 45)\n",
        "plt.yticks(range(len(y_axis_label)), y_axis_label, fontsize=15)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7292462",
      "metadata": {
        "id": "d7292462"
      },
      "source": [
        "## Problem 4: TransSelf Attention (14 pts)\n",
        "- In this problem, you will implement the query-key-value calculation that was used for Transformer\n",
        "- Also, you have to implement self-attention and cross-attention, which are the core components of Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c2c04",
      "metadata": {
        "id": "767c2c04"
      },
      "source": [
        "### 4-1 Implement Query-Key-Value Calculation (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fb7765",
      "metadata": {
        "id": "35fb7765"
      },
      "outputs": [],
      "source": [
        "def get_query_key_value(input_tensor, qkv_layer):\n",
        "  '''\n",
        "  This function returns key, query, and value that is calculated by input tensor and nn_layer.\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "\n",
        "  Outputs:\n",
        "    queries (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    keys (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    values (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.chunk() to split a tensor into given number of chunks\n",
        "  '''\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(4, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "queries, keys, values = get_query_key_value(test, linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c0cae6",
      "metadata": {
        "id": "82c0cae6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 0.5393,  0.0587,  0.6597, -1.1150, -0.7343,  0.3282,  0.0551,  0.0178,\n",
        "         0.4408, -0.3078,  0.3289, -0.4874,  0.2256, -0.1007, -0.4304, -0.2109])\n",
        "answer2 = torch.Tensor([ 0.8704, -0.2256,  0.6611,  0.0332, -0.5233, -0.1159,  0.1805,  0.7238,\n",
        "         0.5590,  0.7260,  1.3096,  0.2465,  1.1961,  0.1751, -0.9674,  0.6297])\n",
        "assert keys.ndim == queries.ndim == values.ndim == 3\n",
        "assert keys.shape == queries.shape == values.shape == torch.Size([4, 17, 16])\n",
        "assert not (keys==queries).any() and not (keys==values).any() and not (values==queries).any()\n",
        "assert torch.allclose(queries[2, 13], answer, atol=1e-4)\n",
        "assert torch.allclose(values[0, 3], answer2, atol=1e-4)\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7cd9187",
      "metadata": {
        "id": "f7cd9187"
      },
      "source": [
        "### 4-2 Implement 3d masked softmax (1 pts):\n",
        "  - This would be almost similar to ``get_masked_softmax()``\n",
        "  - It is common to use N x Tq x Tk attention score and mask, but we will use N x Tk x Tq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8167e8a2",
      "metadata": {
        "id": "8167e8a2"
      },
      "outputs": [],
      "source": [
        "def get_3d_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Tk, Tq]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Tk, Tq] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, tk, tq] == 1 if and only if input_batch[n,tk] is not a padded value.\n",
        "                         If input_batch[n,tk] is a padded value, then mask[n,tk, tq] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Tk, Tq]\n",
        "\n",
        "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "\n",
        "  '''\n",
        "  assert attention_score.ndim == 3 and mask.ndim == 3\n",
        "\n",
        "  return\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "torch.manual_seed(0)\n",
        "mask = torch.ones([3, 9, 9])\n",
        "mask[1, 2:] = 0\n",
        "mask[2, 7:] = 0\n",
        "att_score = torch.randn([3, 9, 9])\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[1, 2:] = 0\n",
        "attention_weight = get_3d_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_3d_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6513919a",
      "metadata": {
        "id": "6513919a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([0.1348, 0.1429, 0.2938, 0.0369, 0.0748, 0.0577, 0.2591, 0.0000, 0.0000])\n",
        "\n",
        "assert attention_weight.ndim == 3\n",
        "# assert torch.allclose(attention_weight[2,:, 0], answer, atol=1e-4)\n",
        "assert not torch.isnan(attention_weight).any(), \"Error: There is a nan value in attention_weight\"\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified, atol=1e-4), \"Error: The attention_weight are different even though only masked item is different\"\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddeea3c2",
      "metadata": {
        "id": "ddeea3c2"
      },
      "source": [
        "### 4-3 Implement Self-Attention (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc503bd",
      "metadata": {
        "id": "7dc503bd"
      },
      "outputs": [],
      "source": [
        "def get_self_attention(input_tensor, qkv_layer, mask):\n",
        "  '''\n",
        "  This function returns output of self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "    mask (torch.Tensor):\n",
        "\n",
        "  Outputs:\n",
        "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function using your completed functions of below:\n",
        "        get_query_key_value()\n",
        "        get_attention_score_for_a_batch_multiple_query()\n",
        "        get_3d_masked_softmax()\n",
        "        get_batch_weighted_sum()\n",
        "  '''\n",
        "  return\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "att_vecs = get_self_attention(test, linear, mask)\n",
        "modified_test = test.clone()\n",
        "modified_test[2, 4:] = 0\n",
        "modified_test[4, 14:] = 0\n",
        "modified_att_vecs = get_self_attention(modified_test, linear, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602218f9",
      "metadata": {
        "id": "602218f9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.3925, -0.0043,  0.0343, -0.6713,  0.2388, -0.4703, -0.2195, -0.1550,\n",
        "        -0.0830, -0.4170, -0.1829,  0.3884,  0.2899,  0.1284,  0.0225, -0.5960])\n",
        "answer2 = torch.Tensor([-0.4078,  0.0173,  0.2670, -0.7959, -0.0314, -0.3455,  0.5751, -0.5806,\n",
        "        -0.3328, -0.2571, -0.4913, -0.1833,  0.6236, -0.5167,  0.3256, -0.9818])\n",
        "assert torch.allclose(att_vecs[3, 2], answer, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[0, 11], answer2, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[4, :14], modified_att_vecs[4, :14], atol=1e-6)\n",
        "\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b23835",
      "metadata": {
        "id": "a1b23835"
      },
      "source": [
        "### 4-4 Implement Multi-head split and concat (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837eeac1",
      "metadata": {
        "id": "837eeac1"
      },
      "outputs": [],
      "source": [
        "def get_multihead_split(x, num_head):\n",
        "  '''\n",
        "  This function returns split tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "    num_head (int): Number of heads\n",
        "\n",
        "  Output:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    The order of N * num_head is [Batch1_head1, Batch1_head2, ..., Batch1_headN, Batch2_head1, Batch2_head2, ..., Batch2_headN, ...]\n",
        "  '''\n",
        "  assert x.shape[-1] % num_head == 0\n",
        "\n",
        "  # TODO: Complete this function\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dummy_input = torch.randn(4, 17, 32)\n",
        "head_split_output = get_multihead_split(dummy_input, 8)\n",
        "head_split_output.shape\n",
        "\n",
        "assert head_split_output.shape == torch.Size([32, 17, 4])\n",
        "assert (dummy_input[0, :, 4:8] == head_split_output[1]).all()\n",
        "assert (dummy_input[0, 3, 8:12] == head_split_output[2, 3, :]).all()\n",
        "assert (dummy_input[2, 10, 16:20] == head_split_output[20, 10, :]).all()\n",
        "print('Passed all the cases!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ac7f68",
      "metadata": {
        "id": "b0ac7f68"
      },
      "outputs": [],
      "source": [
        "def get_multihead_concat(x, num_head):\n",
        "  '''\n",
        "  This function returns concat tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    num_head (int): Number of heads\n",
        "  Outputs:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "  '''\n",
        "  # TODO: Complete this function\n",
        "  assert x.shape[0] % num_head == 0\n",
        "\n",
        "  return\n",
        "\n",
        "head_cat_output = get_multihead_concat(head_split_output, 8)\n",
        "print(f\"Output shape: {head_cat_output.shape}\")\n",
        "assert head_cat_output.shape == torch.Size([4, 17, 32])\n",
        "assert (dummy_input == head_cat_output).all()\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff032b3",
      "metadata": {
        "id": "6ff032b3"
      },
      "source": [
        "### 4-5 Implement Transformer-like multi-head self-attention (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b655a3",
      "metadata": {
        "id": "b7b655a3"
      },
      "outputs": [],
      "source": [
        "def get_multi_head_self_attention(input_tensor, qkv_layer, output_proj_layer, mask, num_head=8):\n",
        "  '''\n",
        "  This function returns output of multi-headed self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    qkv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "    output_proj_layer (torch.nn.Linear): Linear layer with in_features=Cn and out_features=C\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, T, T] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "    num_head (int): Number of heads\n",
        "\n",
        "  Outputs:\n",
        "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function using your completed functions of below:\n",
        "        get_query_key_value(): Get QKV from input_tensor and kqv_layer\n",
        "\n",
        "        get_multihead_split(): Split QKV into multiple heads\n",
        "\n",
        "        get_attention_score_for_a_batch_multiple_query(): Get attention score for a batch of multiple queries\n",
        "          CAUTION: You have to scale the attention score by dividing by sqrt(Cn // num_head)\n",
        "          HINT: att_score /= keys.shape[-1] ** 0.5\n",
        "\n",
        "        get_3d_masked_softmax(): Get masked softmax/\n",
        "          CAUTION: You have to repeat mask for num_head times to use it for multi-head attention\n",
        "          USE head_repeated_mask\n",
        "        get_batch_weighted_sum(): Get batch weighted sum\n",
        "\n",
        "        get_multihead_concat(): Concatenate multiple heads into a single tensor\n",
        "\n",
        "        Additionally, use output_proj_layer to project concatenated tensor at the final step\n",
        "  '''\n",
        "  head_repeated_mask = mask.unsqueeze(1).repeat(1, num_head, 1, 1).reshape(-1, mask.shape[1], mask.shape[2])\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 16)\n",
        "linear = nn.Linear(16, 16 * 3)\n",
        "out_proj = nn.Linear(16, 16)\n",
        "\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "att_vecs = get_multi_head_self_attention(test, linear, out_proj, mask, num_head=4)\n",
        "att_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a3d6ac",
      "metadata": {
        "id": "c7a3d6ac"
      },
      "outputs": [],
      "source": [
        "wrong_answer = torch.tensor([ 0.0105,  0.2337,  0.1587, -0.0346,  0.1446,  0.0231,  0.0296, -0.0437,\n",
        "        -0.1101,  0.2079, -0.0813, -0.2343, -0.3270,  0.2156, -0.0427, -0.0313])\n",
        "wrong_answer2 = torch.tensor([ 0.0557,  0.1026, -0.0834,  0.1770, -0.4083, -0.1543,  0.2025, -0.1570,\n",
        "         0.0974, -0.0317,  0.0569, -0.0610,  0.0063,  0.0107, -0.0727,  0.0796])\n",
        "\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer, atol=1e-4), \"Error: You did not include attention score scaling. Read the CAUTION!\"\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer2, atol=1e-4), \"Error: You did not include output projection. Read the TODO!\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422a8e6d",
      "metadata": {
        "id": "422a8e6d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Compare with official implementation of PyTorch\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(16, num_heads=4, batch_first=True)\n",
        "official_attention.in_proj_weight.data = linear.weight.data\n",
        "official_attention.in_proj_bias.data = linear.bias.data\n",
        "official_attention.out_proj.weight.data = out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = out_proj.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 4, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, attn_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(att_vecs, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169acb5",
      "metadata": {
        "id": "c169acb5"
      },
      "source": [
        "### 4-6 Implement it as a single module (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48M0cSsLW8DA",
      "metadata": {
        "id": "48M0cSsLW8DA"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.qkv = nn.Linear(self.input_size, self.hidden_size * 3)\n",
        "    self.out_proj = nn.Linear(self.hidden_size, self.input_size)\n",
        "    self.mask_value = mask_value\n",
        "    self.num_head = num_head\n",
        "    assert self.hidden_size % self.num_head == 0\n",
        "    self.dim_per_head = self.hidden_size // self.num_head\n",
        "\n",
        "  '''\n",
        "  TODO: Implement this function as functions you implemented above\n",
        "  '''\n",
        "  def _get_qkv(self, x):\n",
        "    return\n",
        "\n",
        "  def _get_multihead_split(self, x):\n",
        "    return\n",
        "\n",
        "  def _get_multiheaded_att_score(self, keys, queries):\n",
        "    return\n",
        "\n",
        "  def _get_masked_softmax(self, score, masks):\n",
        "    return\n",
        "\n",
        "  def _get_weighted_sum(self, values, weights):\n",
        "    return\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    '''\n",
        "    TODO: Implement this function using the functions you implemented above\n",
        "    '''\n",
        "    if mask is None:\n",
        "      mask = torch.ones([x.shape[0], x.shape[1], x.shape[1]])\n",
        "\n",
        "    return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "attention_module = SelfAttention(512, 512, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "out = attention_module(test, mask)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0ed205",
      "metadata": {
        "id": "ef0ed205"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(512, num_heads=8, batch_first=True)\n",
        "official_attention.in_proj_weight.data = attention_module.qkv.weight.data\n",
        "official_attention.in_proj_bias.data = attention_module.qkv.bias.data\n",
        "official_attention.out_proj.weight.data = attention_module.out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = attention_module.out_proj.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, attn_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(out, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc74bbbf",
      "metadata": {
        "id": "cc74bbbf"
      },
      "source": [
        "### Test your SelfAttention module on Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac3e753",
      "metadata": {
        "id": "bac3e753"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.input_size = in_size\n",
        "    self.layer = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(hidden_size, in_size))\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class PosEncoding(nn.Module):\n",
        "  def __init__(self, size, max_t):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.max_t = max_t\n",
        "    self.register_buffer('encoding', self._prepare_emb())\n",
        "\n",
        "  def _prepare_emb(self):\n",
        "    dim_axis = 10000**(torch.arange(self.size//2) * 2 / self.size)\n",
        "    timesteps = torch.arange(self.max_t)\n",
        "    pos_enc_in = timesteps.unsqueeze(1) / dim_axis.unsqueeze(0)\n",
        "    pos_enc_sin = torch.sin(pos_enc_in)\n",
        "    pos_enc_cos = torch.cos(pos_enc_in)\n",
        "\n",
        "    pos_enc = torch.stack([pos_enc_sin, pos_enc_cos], dim=-1).reshape([self.max_t, 512])\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.encoding[x]\n",
        "\n",
        "class ResidualLayerNormModule(nn.Module):\n",
        "  def __init__(self, submodule):\n",
        "    super().__init__()\n",
        "    self.submodule = submodule\n",
        "    self.layer_norm = nn.LayerNorm(self.submodule.input_size)\n",
        "\n",
        "  def forward(self, x, mask=None, y=None):\n",
        "    if y is not None:\n",
        "      res_x = self.submodule(x, y, mask)\n",
        "    elif mask is not None:\n",
        "      res_x = self.submodule(x, mask)\n",
        "    else:\n",
        "      res_x = self.submodule(x)\n",
        "    x =  x + res_x\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.mlp_block(self.att_block(x['input'], x['mask']))\n",
        "    return {'input':out, 'mask':x['mask']}\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "encoder_layer = EncoderLayer(512, 512, 2048, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "out = encoder_layer({'input':test, 'mask':mask})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb07b11b",
      "metadata": {
        "id": "bb07b11b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_encoder_layer = nn.TransformerEncoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "\n",
        "official_encoder_layer.self_attn.in_proj_weight.data = encoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_encoder_layer.self_attn.in_proj_bias.data = encoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_encoder_layer.self_attn.out_proj.weight.data = encoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_encoder_layer.self_attn.out_proj.bias.data = encoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_encoder_layer.linear1.weight.data = encoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_encoder_layer.linear1.bias.data = encoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_encoder_layer.linear2.weight.data = encoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_encoder_layer.linear2.bias.data = encoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_encoder_layer.norm1.weight.data = encoder_layer.att_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm1.bias.data = encoder_layer.att_block.layer_norm.bias.data\n",
        "official_encoder_layer.norm2.weight.data = encoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm2.bias.data = encoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_encoder_output = official_encoder_layer(test, src_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(official_encoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test case!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b169486",
      "metadata": {
        "id": "1b169486"
      },
      "source": [
        "### 4-7 Implement Transformer-like multi-head cross-attention (2 pts)\n",
        "- Implement ``CrossAttention`` inheriting `SelfAttention` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b8a722",
      "metadata": {
        "id": "b5b8a722"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(SelfAttention):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__(input_size, hidden_size, num_head, mask_value)\n",
        "\n",
        "  def forward(self, q_seq, kv_seq, mask=None):\n",
        "    '''\n",
        "    Arguments:\n",
        "      q_seq (torch.Tensor): Sequence to be used for query\n",
        "      kv_seq (torch.Tensor): Sequence to be used for key and value\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ty, Tx]\n",
        "\n",
        "    Outs:\n",
        "      output (torch.Tensor): Output of cross attention. Shape: [N, Tx, C]\n",
        "\n",
        "    TODO: Complete this function using your completed functions of below:\n",
        "    '''\n",
        "    if mask is None:\n",
        "      mask = torch.ones([q_seq.shape[0], kv_seq.shape[1], q_seq.shape[1]])\n",
        "\n",
        "    return\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.cross_att_block = ResidualLayerNormModule(CrossAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.att_block(x['input'], x['decoder_mask'])\n",
        "    out = self.cross_att_block(out,  x['encoder_mask'], x['encoder_out'])\n",
        "    out = self.mlp_block(out)\n",
        "    return {'input':out, 'decoder_mask':x['decoder_mask'], 'encoder_out':x['encoder_out'], 'encoder_mask':x['encoder_mask']}\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "decoder_layer = DecoderLayer(512, 512, 2048, 8)\n",
        "test_src = torch.randn(5, 17, 512)\n",
        "test_tgt = torch.randn(5, 19, 512)\n",
        "mask_src = torch.ones([5, 17, 19])\n",
        "mask_src[2, 4:] = 0\n",
        "mask_src[4, 14:] = 0\n",
        "mask_tgt = torch.tril(torch.ones(test_tgt.shape[0], test_tgt.shape[1], test_tgt.shape[1]))\n",
        "\n",
        "out = decoder_layer({'input':test_tgt, 'decoder_mask':mask_tgt, 'encoder_out':test_src, 'encoder_mask':mask_src})\n",
        "out['input'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50ff7d",
      "metadata": {
        "id": "fd50ff7d"
      },
      "outputs": [],
      "source": [
        "official_decoder_layer = nn.TransformerDecoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "official_decoder_layer.self_attn.in_proj_weight.data = decoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.self_attn.in_proj_bias.data = decoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.self_attn.out_proj.weight.data = decoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.self_attn.out_proj.bias.data = decoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.multihead_attn.in_proj_weight.data = decoder_layer.cross_att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.multihead_attn.in_proj_bias.data = decoder_layer.cross_att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.multihead_attn.out_proj.weight.data = decoder_layer.cross_att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.multihead_attn.out_proj.bias.data = decoder_layer.cross_att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.linear1.weight.data = decoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_decoder_layer.linear1.bias.data = decoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_decoder_layer.linear2.weight.data = decoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_decoder_layer.linear2.bias.data = decoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_decoder_layer.norm1.weight.data = decoder_layer.att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm1.bias.data = decoder_layer.att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm2.weight.data = decoder_layer.cross_att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm2.bias.data = decoder_layer.cross_att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm3.weight.data = decoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm3.bias.data = decoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "# Mask for self-attentionq\n",
        "# In nn.TransformerEncoderLayer or nn.TransformerDecoderLayer, the mask is expected to be [N, Tq, Tk]\n",
        "# where N is the batch size, Tt is the query sequence length, and Ts is the key sequence length.\n",
        "head_repeated_mask_src = mask_src.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask_src.shape[1], mask_src.shape[2]).transpose(1,2)\n",
        "head_repeated_mask_tgt = mask_tgt.unsqueeze(1).repeat(1,8,1,1).reshape(-1, mask_tgt.shape[1], mask_tgt.shape[2]).transpose(1,2)\n",
        "official_decoder_output = official_decoder_layer(test_tgt, test_src, tgt_mask=head_repeated_mask_tgt==0, memory_mask=head_repeated_mask_src==0)\n",
        "\n",
        "assert torch.allclose(official_decoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d895a7da",
      "metadata": {
        "id": "d895a7da"
      },
      "source": [
        "### Using your implementation, we can build Transformer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266428cc",
      "metadata": {
        "id": "266428cc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(EncoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = torch.ones([x.shape[0], x.shape[1], x.shape[1]])\n",
        "    mask[x==0] = 0\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'mask':mask})\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(DecoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    mask = torch.triu(torch.ones(x.shape[0], x.shape[1], x.shape[1]))\n",
        "    cross_attention_mask = torch.ones(x.shape[0], y['input'].shape[1], x.shape[1]) # N, Tk, Tq\n",
        "    cross_attention_mask[y['mask'][:,:, 0]==0] = 0\n",
        "\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'decoder_mask':mask, 'encoder_out':y['input'], 'encoder_mask':cross_attention_mask})\n",
        "\n",
        "class TransformerTranslator(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_enc_layers, num_dec_layers, enc_vocab_size, dec_vocab_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(in_size, emb_size, mlp_size, num_head, num_enc_layers, enc_vocab_size)\n",
        "    self.decoder = Decoder(in_size, emb_size, mlp_size, num_head, num_dec_layers, dec_vocab_size)\n",
        "    self.final_proj = nn.Linear(emb_size, dec_vocab_size)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, y:torch.Tensor):\n",
        "    '''\n",
        "    Arguments:\n",
        "    '''\n",
        "    enc_out = self.encoder(x)\n",
        "    dec_out = self.decoder(y, enc_out)\n",
        "    return self.final_proj(dec_out['input']).softmax(dim=-1)\n",
        "\n",
        "class TransformerTrainer(Trainer):\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    super().__init__(model, optimizer, loss_fn, train_loader, valid_loader, device)\n",
        "    self.num_iter = 0\n",
        "    self._adjust_optim()\n",
        "\n",
        "  def _adjust_optim(self):\n",
        "    self.num_iter += 1\n",
        "    self.optimizer.param_groups[0]['lr'] = 512 ** (-0.5) * min(self.num_iter**(-0.5), self.num_iter*4000**(-1.5))\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (Translator/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "\n",
        "    src, tgt_i, tgt_o = batch\n",
        "    pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "    pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "    loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "    self._adjust_optim()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    TODO: Complete this method\n",
        "\n",
        "    '''\n",
        "\n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt_i, tgt_o = batch\n",
        "        tgt_o = tgt_o.to(self.device)\n",
        "        pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "        pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "        loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(pred, tgt_o)\n",
        "\n",
        "        validation_loss += loss.item() * len(pred.data)\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          num_correct_guess += (pred.data.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (pred.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        num_data += len(pred.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "def pad_collate(raw_batch):\n",
        "  srcs = [x[0] for x in raw_batch]\n",
        "  tgts_i = [x[1][:-1] for x in raw_batch]\n",
        "  tgts_o = [x[1][1:] for x in raw_batch]\n",
        "\n",
        "  srcs = pad_sequence(srcs, batch_first=True)\n",
        "  tgts_i = pad_sequence(tgts_i, batch_first=True)\n",
        "  tgts_o = pack_sequence(tgts_o, enforce_sorted=False)\n",
        "  return srcs, tgts_i, tgts_o\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=4, collate_fn=pad_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "tfm_model = TransformerTranslator(512,512,2048,8,6,12,32000,32000)\n",
        "out = tfm_model(batch[0], batch[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42f7c9b",
      "metadata": {
        "id": "c42f7c9b"
      },
      "source": [
        "#### Transformer Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c32d97",
      "metadata": {
        "id": "36c32d97"
      },
      "outputs": [],
      "source": [
        "# download pre-trained weight\n",
        "!gdown 1nxbQPD4a2SVicLBfzrj2pRU4hAapeMmL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40d8308",
      "metadata": {
        "id": "e40d8308"
      },
      "outputs": [],
      "source": [
        "tfm_model = TransformerTranslator(512,512,2048,8,6,12,32000,32000)\n",
        "tfm_model.load_state_dict(torch.load('kor_eng_translator_tfm_model_best.pt')['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec607d0b",
      "metadata": {
        "id": "ec607d0b"
      },
      "outputs": [],
      "source": [
        "def translate_tfm(model, source_sentence, src_tokenizer, tgt_tokenizer):\n",
        "  '''\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  enc_out = model.encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "\n",
        "  for i in range(50): # You can chage it to while True:\n",
        "    decoder_out = model.decoder(current_decoder_token, enc_out)['input']\n",
        "    logit = model.final_proj(decoder_out[0, -1])\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = torch.tensor(current_decoder_token[0].tolist() + [selected_token], dtype=torch.long).unsqueeze(0)\n",
        "    if selected_token == 3: ## end of sentence token\n",
        "      break\n",
        "  predicted_tokens = current_decoder_token.squeeze().tolist()[1:-1]\n",
        "  return input_tokens, predicted_tokens, tgt_tokenizer.decode(predicted_tokens)\n",
        "\n",
        "tfm_model.cpu()\n",
        "tfm_model.eval()\n",
        "input_tokens, pred_tokens, translated_string  = translate_tfm(tfm_model, '문장이 잘 번역되는지를 통해 모델 설계와 학습이 잘 이뤄졌는지를 확인할 수 있습니다.', src_tokenizer, tgt_tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8dda25",
      "metadata": {
        "id": "ce8dda25"
      },
      "source": [
        "# Check Before Submission\n",
        "- Copy and paste your code to the downloaded ``Assignment4.py``\n",
        "  - https://raw.githubusercontent.com/jdasam/aat3020-2023/main/Assignment4.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ec2b52",
      "metadata": {
        "id": "29ec2b52"
      },
      "outputs": [],
      "source": [
        "# Run this code after copy and paste your code to Assignment4.py\n",
        "!python Assignment4.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}